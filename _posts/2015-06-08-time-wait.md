---
layout: post
title: What is TIME_WAIT state?
---

<div class="message">
TIME_WAIT 상태가 늘어나면 서버의 소켓이 고갈되어 커넥션 타임아웃이 발생한다는 얘기를 한다. 이 말이 정확한 얘기인지, TIME_WAIT은 어떠한 경우에 발생하고 어떤 특징이 있는지 살펴본다.
</div>

*2015년 6월 4일 초안 작성*

## 내용

### TIME_WAIT 상태

`TIME_WAIT` 이란 TCP 상태의 가장 마지막 단계이여, 이미 지난 [CLOSE_WAIT 문서](http://docs.likejazz.com/close-wait/)에서도 살펴본바 있다. **Active Close** 즉, 먼저 `close()`를 요청한 곳에서 최종적으로 남게되며, 2 MSL 동안 유지된다.

---

이 단순한 과정이 매 번 어렵게 느껴지는 이유는 대부분은 고급언어로 소켓을 랩핑해서 사용하기 때문에(이전 문서에서 예제로 제시한 Java 코드[^fn-4]도 `accept()` 이전 모든 과정을 라이브러리가 랩핑해서 처리) 소켓에 문제가 생기지 않는한 로우 레벨로 내려가 확인할 일이 흔치 않고, 또한 확인하는 방법을 아는 이도 매우 드물다.

그러다 보니 인터넷에 잘못된 정보가 넘쳐난다. 

스택오버플로우 포함, 잘못된 정보가 제대로 된 검증과정도 거치지 않은채, 또는 검증할 능력이 부족한 상태에서 계속 인용되면서 잘못된 정보가 지속적으로 확대 재생산된다.

잘못된 정보를 접한 이들은 내용이 서버의 동작과 일치하지 않으니 계속 이해를 못하게 되고 점점 더 어렵게 느껴진다. 그야 말로 '진퇴양난'이다.

현재 시점에서 인터넷에 있는 가장 정확한 문서는 Vincent Bernat 가 작성한 [Coping with the TCP TIME-WAIT state on busy Linux servers](http://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux.html) 이다. 이외 대부분의 문서는 잘못된 내용을 담고 있는 경우가 많으니 주의가 필요하다.

그렇다면 `TIME_WAIT` 상태가 왜 필요하고, 왜 그렇게 길게 설정되어 있는지 이유를 살펴보도록 한다. 두 가지 경우로 정리[^fn-2]할 수 있다.

<img src="http://d1g3mdmxf8zbo9.cloudfront.net/images/tcp/duplicate-segment.png" width="300" />

첫 번째는 지연 패킷이 발생할 경우다.

`TIME_WAIT`이 짧다면 이미 다른 연결로 진행될 수 있고 지연 패킷이 뒤늦게 도달해 문제가 될 수 있다.

<img src="http://d1g3mdmxf8zbo9.cloudfront.net/images/tcp/last-ack.png" width="300" />

두 번째는 원격 종단의 연결이 닫혔는지 확인해야할 경우다. 

`TIME_WAIT`이 짧다면 마지막 `ACK` 유실시 상대방은 `LAST_ACK` 상태에 빠지게 되고 새로운 `SYN` 패킷 전달시 `RST`로, 새로운 연결은 오류를 내며 실패하게 된다.

---

RFC 793 에는 `TIME_WAIT`을 2 MSL로 규정했으며 CentOS 6에서는 60초 동안 유지된다. 아울러 **이 값은 조정할 수 없다**.

`net.ipv4.tcp_fin_timeout`을 설정하면 `TIME_WAIT`의 타임아웃을 변경할 수 있다는 내용이 많은데 이는 **잘못된 정보**다. `TIME_WAIT`의 타임아웃 정보는 커널 헤더 `include/net/tcp.h` 에 하드  코딩[^fn-5] 되어 있으며 변경이 불가능하다.

{% highlight c %}
#define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT
                                  * state, about 60 seconds     */
{% endhighlight %}

### 예제 프로그램

지난 번과 달리 이번에는 소켓의 로우 레벨까지 확인하기 위해 예제 서버 프로그램을 C 로 구현했다. 리눅스를 포함한 모든 유닉스 기반 OS 의 API 가 C 로 구현되어 있고 특히 네트워크 프로그램에서 커널의 동작과 C 의 어플리케이션 API 는 정확히 1:1 로 대응 된다. 따라서 구체적으로 네트워크가 어떻게 동작하는지를 C 로 직접 구현해 하나씩 확인해보도록 한다.

먼저 서버 프로그램은 예전에 커넥션 테스트 용도로 개발해 [깃헙에 공개한 CONTEST 서버](https://github.com/likejazz/contest-server)를 기반으로 일부 코드를 추가해서 구현했다.

{% highlight c %}
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <string.h>
#include <time.h>
#include <pthread.h>
#include <net/if.h>
#include <sys/ioctl.h>

#define BACKLOG       128     // backlog for listen()
#define DEFAULT_PORT  5000    // default listening port
#define BUF_SIZE      1024    // message chunk size

time_t ticks;

typedef struct {
  int client_sock;
} thread_param_t;
thread_param_t *params;       // params structure for pthread

pthread_t thread_id;          // thread id

void error(char *msg) {
  perror(msg);
  exit(EXIT_FAILURE);
}

/**
* thread handler after accept()
*/
void *handle_client(void *params) {
  char msg[BUF_SIZE];
  thread_param_t *p = (thread_param_t *) params;  // thread params

  // clear message buffer
  memset(msg, 0, sizeof(msg));

  ticks = time(NULL);
  snprintf(msg, sizeof(msg), "%.24s\r\n", ctime(&ticks));
  write(p->client_sock, msg, strlen(msg));
  printf("Sent message to Client #%d\n", p->client_sock);

  // wait
  usleep(10);

  // clear message buffer
  memset(msg, 0, sizeof(msg));

  // active close, It will remains in `TIME_WAIT` state.
  if (close(p->client_sock) < 0)
    error("Error: close()");
  free(p);

  return NULL;
}

int main(int argc, char *argv[]) {
  int listenfd = 0, connfd = 0;
  struct sockaddr_in serv_addr;
  struct ifreq ifr;

  // build server's internet addr and initialize
  memset(&serv_addr, '0', sizeof(serv_addr));

  serv_addr.sin_family = AF_INET;                // an internet addr
  serv_addr.sin_addr.s_addr = htonl(INADDR_ANY); // accept any interfaces
  serv_addr.sin_port = htons(DEFAULT_PORT);      // the port we will listen on

  // create the socket, bind, listen
  if ((listenfd = socket(AF_INET, SOCK_STREAM, 0)) < 0)
    error("Error: socket()");
  if (bind(listenfd, (struct sockaddr *) &serv_addr, sizeof(serv_addr)) < 0)
    error("Error: bind() Not enough privilleges(<1024) or already in use");
  if (listen(listenfd, BACKLOG) < 0)
    error("Error: listen()");

  ifr.ifr_addr.sa_family = AF_INET;               // an internet addr
  strncpy(ifr.ifr_name, "eth0", IFNAMSIZ-1);      // IP address attached to "eth0"
  ioctl(listenfd, SIOCGIFADDR, &ifr);             // get ifnet address

  // print <IP>:<Port>
  printf("Listening on %s:%d\n",
         inet_ntoa(((struct sockaddr_in *)&ifr.ifr_addr)->sin_addr),
         DEFAULT_PORT);

  while (1) {
    connfd = accept(listenfd, (struct sockaddr *) NULL, NULL);

    // memory allocation for `thread_param_t` struct
    params = malloc(sizeof(thread_param_t));
    params->client_sock = connfd;

    // 1 client, 1 thread
    pthread_create(&thread_id, NULL, handle_client, (void *) params);
    pthread_detach(thread_id);
  }
}
{% endhighlight %}

Java에 비해 코드는 훨씬 더 길지만 동작 방식에는 큰 차이가 없다. 마찬가지로 서버가 먼저 `close()`를 시도하는 점도 동일하다. 즉, 서버측이 Active Close가 되고 `TIME_WAIT` 상태에 빠지게된다. 

### 서버 동작

서버의 동작은 `socket()` - `bind()` - `listen()` - `accept()` 과정을 거쳐 클라이언트와 연결되며 위 코드에는 그 과정이 상세히 잘 나와 있다.

`accept()` 이후 서버는 `listen()` 중인 소켓과 별도로 `accept()` 를 위한 소켓을 추가로 생성해 클라이언트를 할당하며, 서버가 클라이언트를 관리하는 방식은 크게 4가지로 구분할 수 있다.[^fn-1]

1. 요청 당 프로세스 할당, `fork()`로 자식 프로세스를 만들어 클라이언트를 담당한다. 전통적인 **blocking I/O** 방식이다.
1. 요청 당 쓰레드 할당. 위 예제에서 사용한 방식으로 `pthread_create()`를 통해 쓰레드를 생성, 클라이언트를 담당한다. 마찬가지로 **blocking I/O** 방식이다.
1. 쓰레드풀을 구성해 각각의 쓰레드가 여러 커넥션을 **asynchronous I/O** 방식으로 담당한다.
1. 쓰레드풀을 구성해 각각의 쓰레드가 여러 커렉션을 `select()`, `poll()`, **nonblocking I/O** 같은 이벤트 기반 방식으로 담당한다.

이 중 대용량 처리에 3, 4번이 우세하며 특히 4번이 대세이다. 하지만, 여기서 모두 언급하기엔 지나치게 방대하므로 추후 별도로 정리해보기로 하고 여기선 가장 간편한 방식인 2번, 클라이언트를 각각의 쓰레드에 할당하고 `close()` 될때 쓰레드도 함께 종료되는 방식으로 구현했다.

<img src="https://farm1.staticflickr.com/267/17905712974_17c2eaff1b_b.jpg" width="600" />

`htop`을 이용해 쓰레드 옵션을 켜고(`H` 키), 트리 모드(`t` 키)에서 요청이 있을때마다 쓰레드가 하나씩 생성되는 모습을 직접 확인한 화면이다.

현재 2개의 요청을 처리 중이며, 물론 요청이 끝나면 쓰레드도 함께 종료된다. 만약 프로세스나 쓰레드를 할당하지 않았다면 요청이 끝날때까지 다른 요청은 받지 못하는 말 그대로 blocking 상태가 유지될 것이다.

### TCP 연결 종료

예제 서버를 구동하고 클라이언트에서 FIN/ACK이 잘 전달되는지 확인해본다. 아울러 서버의 Active Close 후 서버측에 남게되는 `TIME_WAIT` 상태를 직접 확인한다.

TCP/IP Illustrated 의 TCP 연결 종료 다이어그램은 아래와 같다.

<img src="https://farm9.staticflickr.com/8896/18339898259_71c350b396_b.jpg" width="500" />

연결 종료의 4-way handshake 과정은 다음과 같다.

- `FIN+ACK`, seq = K, ack = L
- `ACK`, seq = L, ack = K + 1
- `FIN+ACK`, seq = L, ack = K + 1
- `ACK`, seq = K, ack = L + 1

서버 접속 후 `tcpdump` 로 패킷 상태를 덤프한 결과다.

{% highlight bash %}
$ tcpdump -nn -i eth0 '(src 10.15.86.214 or dst 10.15.86.214)'
13:32:31.626549 IP 10.41.147.237.5000 > 10.15.86.214.30202: Flags [F.], seq 27, ack 1, win 16, options [nop,nop,TS val 2599042792 ecr 2329783108], length 0
13:32:31.626877 IP 10.15.86.214.30202 > 10.41.147.237.5000: Flags [.], ack 27, win 6, options [nop,nop,TS val 2329783108 ecr 2599042792], length 0
13:32:31.626964 IP 10.15.86.214.30202 > 10.41.147.237.5000: Flags [F.], seq 1, ack 28, win 6, options [nop,nop,TS val 2329783108 ecr 2599042792], length 0
13:32:31.626975 IP 10.41.147.237.5000 > 10.15.86.214.30202: Flags [.], ack 2, win 16, options [nop,nop,TS val 2599042792 ecr 2329783108], length 0
{% endhighlight %}

포트 5000번이 서버다. 그리고 서버가 먼저 `FIN`을 보내며 **Active Close**를 시도했다. tcpdump 의 결과 Flags, 패킷 타입 플래그는 다음과 같다.

- `[S]` - SYN (Start Connection)
- `[.]` - No Flag Set
- `[P]` - PSH (Push Data)
- `[F]` - FIN (Finish Connection)
- `[R]` - RST (Reset Connection) 

`[.]`은 `ACK`를 뜻하며 `[F.]` 은 `FIN+ACK` 을 가리키는 싱글 패킷이다. 이에 따라 실제 덤프 결과를 분석해보면,

- `FIN+ACK`, seq = 27, ack = 1
- `ACK`, seq = 1, ack = 27
- `FIN+ACK`, seq = 1, ack = 28
- `ACK`, seq = 28, ack = 2

verbose 모드가 아니었기 때문에 `ACK`의 seq는 보이지 않는다. 추가로 `-vv` 옵션을 부여하면 verbose 모드가 되며 확인할 수 있다.

그런데 CentOS 6의 결과 중 두 번째 `ACK`의 ack 값과 마지막 `ACK`의 seq 값이 TCP/IP Illustrated 에서 명시된 숫자와 하나씩 다르다. OS 별로 커널 버전 별로 조금씩 다르게 동작하기도 하는데 이 부분은 추후에 보다 정확한 확인이 필요하다.

이렇게 종료 handshake 과정이 정상적으로 끝나면 **Active Close**를 먼저 요청한 서버쪽에 `TIME_WAIT`이 남게된다.

<img src="https://farm9.staticflickr.com/8858/18532247925_9d15cf4ff6_b.jpg" width="650" />

처리한 쓰레드도 이미 종료된 상태로 할당된 프로세스도 보이지 않는다. 이미 커널로 처리 권한이 넘어 갔으며 프로세스를 종료해도 `TIME_WAIT` 상태는 사라지지 않는다. 오히려 소켓이 해당 포트를 점유하고 있는 상태로 60초 동안 재시작을 할 수 없게 된다.

재시작을 시도하면 `Address already in use`로 `bind()` 단계에서 오류가 발생하며 시간이 지나 `TIME_WAIT`이 모두 사라진 후에야 가능하다.

만약 즉시 재시작이 필요하다면 `setsockopt()`의 `SO_REUSEADDR` 옵션을 적용하면 `bind()` 단계에서 커널이 가져간 처리 권한을 다시 돌려받으며 즉시 재시작 가능하다.

{% highlight c %}
setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &yes, sizeof(yes));
{% endhighlight %}

참고로 `SO_REUSEADDR`가 `tcp_tw_reuse`와 동일한 역할을 한다는 문서가 있는데 **잘못된 정보**다. `SO_REUSEADDR`은 소켓 바인딩시 해당 포트의 네트워크 상태를 커널에서 즉시 돌려 받는 것이고, `tcp_tw_reuse`는 새로운 연결에 `TIME_WAIT` 상태를 재사용하는 것이다. 전자는 서버의 역할이고, 후자는 클라이언트의 역할로 전혀 다르다. 

또한 클라이언트 소켓에 `SO_REUSEADDR`을 부여한다고 `TIME_WAIT`을 재사용할 수 있는게 아니다. 반드시 `tcp_tw_reuse` 커널 설정이 필요하다. 이후에 다시 자세히 설명한다.

### 자원 할당

그렇다면 `TIME_WAIT`은 시스템 성능 저하를 가져올까. 

> Each socket in TIME_WAIT consumes some memory in the kernel, usually somewhat less than an ESTABLISHED socket yet still significant. A sufficiently large number could exhaust kernel memory, or at least degrade performance because that memory could be used for other purposes.[^fn-7]

> Because application protocols do not take TIME-WAIT TCB distribution into account, heavily loaded servers can have thousands of connections in TIME-WAIT that consume memory and can slow active connections. In BSD-based TCP implementations, TCBs are kept in mbufs, the memory allocation unit of the networking subsystem[9]. There are a finite number of mbufs available in the system, and mbufs consumed by TCBs cannot be used for other purposes such as moving data. Some systems on high speed networks can run out of mbufs due to TIME-WAIT buildup under high connection load. A SPARCStation 20/71 under SunOS 4.1.3 on a 640 Mb/s Myrinet[10] cannot support more than 60 connections/sec because of this limit.[^fn-6]

커널 메모리 공간을 점유하기 때문에 성능 저하를 야기할 수 있다고 하는데 실제로 그런지 고부하 테스트를 통해 확인해보기로 한다.

<img src="https://farm9.staticflickr.com/8891/18532873405_6cf6227ca9_b.jpg" width="250" />

십수여대의 클라이언트를 동원해 연속된 요청으로 9만개 가까운 `TIME_WAIT` 상태를 만들어 냈다.

{% highlight bash %}
$ ss -ant | awk '{print $1}' | grep -v '[a-z]' | sort | uniq -c
      1 ESTAB
     15 LISTEN
  89262 TIME-WAIT
{% endhighlight %}

서버의 소켓 수는 할당 가능한 로컬 포트 만큼인 최대 65K로 언급하고 `net.ipv4.ip_local_port_range` 설정으로 변경할 수 있다고 언급하는 문서가 있는데 마찬가지로 **잘못된 정보**다. 서버는 로컬 포트를 사용하지 않는다.

만약 서버가 로컬 포트를 사용한다면 위 처럼 9만개 가까운 `TIME_WAIT`을 만들어 낼 수 없다. 서버가 할당하는 것은 포트가 아닌 **소켓**이며 서버의 포트는 최초 `bind()`시 하나만 사용하게 된다. 

로컬 포트를 할당하는 것은 클라이언트이며, 클라이언트가 `connect()`시 로컬 포트를 임의로 바인딩하면서 서버의 소켓과 연결된다.

소켓은 protocol, src addr, src port, dest addr, dest port 5개 값이 유니크하게 구성된다. 따라서 서버 포트가 추가되거나 클라이언트의 IP가 추가될 경우 그 만큼의 새로운 쌍을 생성할 수 있어 `TIME_WAIT`가 많이 남아 있어도 별 문제가 없다.

서버가 또 다른 서버에 클라이언트로써 접속하지만 않는다면 자신의 로컬 포트는 사용할 일이 없으며, 기본적으로 리눅스의 로컬 포트 수는 3만개로 설정 되어 있다. 만일 서버 투 서버로 1:1 대용량 접속이 발생할 경우 클라이언트의 최대 요청 수는 500 RPS 까지 가능하다는 얘기다. 이 수치를 넘어서지만 않는다면 아무런 커널 설정도 변경할 필요가 없다.

### TIME_WAIT 재사용

`TIME_WAIT` 재사용이 필요한 경우는 클라이언트의 로컬 포트가 고갈될 경우이며 아래 3가지 경우로 분류할 수 있다.

1. 서버에 `TIME_WAIT` 상태가 남아 있으며, 클라이언트의 로컬 포트는 고갈된 경우
1. 클라이언트에 `TIME_WAIT` 상태가 남아 있으며, 클라이언트의 로컬 포트는 고갈된 경우
1. 클라이언트에 `TIME_WAIT` 상태가 남아 있으며, 클라이언트의 로컬 포트는 고갈되고, 서버의 다른 포트에 접속할 경우

< 예정 >

Do not enable net.ipv4.tcp_tw_recycle

### 소켓 링거

< 예정 >

### C10K Problem

< 예정 >

## 정리

TCP/IP Illustrated를 쓴 리차드 스티븐스의 또 다른 책 Unix Network Programming에는 이런 구절[^fn-2]이 있다.

> The `TIME_WAIT` state is our friend and is there to help us (i.e., to let old duplicate segments expire in the network). Instead of trying to avoid the state, we should understand it.

`TIME_WAIT`은 우리를 도와주는 우리의 친구다. 네트워크에서 오래된 중복 세그먼트를 날려주는 훌륭한 역할을 한다. 자꾸만 없애려고 노력하지 말고 이해해야 한다.

---

수 많은 잘못된 정보들 사이에서 아래와 같은 올바른 정보를 반드시 기억해두길 바란다.

- `TIME_WAIT`의 타임아웃은 60초로 하드코딩[^fn-5]되어 있다. 설정할 수 없다.
- 다수의 `TIME_WAIT`이 서버 성능을 저하시킨다는 논문[^fn-6]은 1997년에 출간됐다. 지금은 2015년이다. 그 당시 서버는 512MB 메모리였고, 지금 이 문서를 작성하며 사용한 서버는 64G 메모리를 갖고 있다.
- `TIME_WAIT`을 재사용하는 `tcp_tw_reuse` 옵션이 필요한 것은 클라이언트 뿐이다.
- `SO_REUSEADDR`는 `tcp_tw_reuse`와 다르다.
- 서버가 클라이언트를 `accept()` 할때 사용하는 것은 소켓 뿐이다. 로컬 포트가 아니다. 서버는 단 하나의 포트만 있어도 된다.
- 클라이언트는 `connect()`시 임의의 로컬 포트(ephemeral port)를 할당한다.
- 소켓의 최대 갯수는 64K가 아니다. 소켓은 protocol, src addr, src port, dest addr, dest port 이 5개의 값으로 유니크하게 구성된다. 서버의 포트는 하나이고 클라이언트의 포트는 계속 변한다. 만약 서버의  포트가 추가되거나 클라이언트의 IP가 추가될 경우 다시 그 만큼의 새로운 쌍을 생성할 수 있다.

### TL;DR

- **서버**는 아무것도 할 필요가 없다.
- **클라이언트**[^fn-3]는 `net.ipv4.tcp_tw_reuse`를 1로 설정한다.

### References

[^fn-1]: <http://www.quora.com/What-is-the-ideal-design-for-server-process-in-Linux-that-handles-concurrent-socket-I-O>
[^fn-2]: <http://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux.html>
[^fn-3]: 여기서 말하는 클라이언트란 일반적인 클라이언트가 아니라, 서버 투 서버로 대용량으로 접속하는 클라이언트를 말한다.
[^fn-4]: <http://docs.likejazz.com/close-wait/>
[^fn-5]: [Line in the linux source](https://github.com/torvalds/linux/blob/f5ddcbbb40aa0ba7fbfe22355d287603dbeeaaac/include/net/tcp.h#L114-L115)
[^fn-6]: <http://www.isi.edu/touch/pubs/infocomm99/infocomm99-web/>
[^fn-7]: <http://stackoverflow.com/a/1854196>