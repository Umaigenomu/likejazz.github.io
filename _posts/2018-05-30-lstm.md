---
layout: post
title: LSTM의 원리와 수식 계산
tags: [Machine Learning]
---

<div class="message">
LSTM은 RNN의 핵심을 이루는 모델이다. 그러나 의외로 실제로 계산할 수 있는 이가 많지 않으며, 개념적으로 잘못 이해하고 있는 분들도 많다. 원리를 정확히 이해하고 계산하는 방법은 항상 중요하므로, 여기서는 원리와 수식을 소개하고 직접 계산을 진행하여 결과가 일치하는지 확인해본다.
</div>

<small>
*2018년 5월 30일 초안 작성*  
</small>

<!-- TOC -->

- [본문](#본문)
    - [LSTM](#lstm)
        - [수식 구현](#수식-구현)
    - [Keras 구현](#keras-구현)
        - [가중치 추출](#가중치-추출)
        - [Hard Sigmoid](#hard-sigmoid)
        - [계산 결과 비교](#계산-결과-비교)
- [코드](#코드)
- [References](#references)

<!-- /TOC -->

## 본문
### LSTM
LSTM을 소개하는 가장 훌륭한 자료로 [Chris Olah의 글](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)이 손꼽힌다. 벌써 3년전 글 임에도 복잡한 LSTM 구조를 가장 이해하기 쉽도록 탁월하게 표현해냈다.

![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)[^fn-1]

LSTM 셀을 떼어내 레이블을 부여해 정리[^fn-2]하면 아래와 같다.

![](https://user-images.githubusercontent.com/1250095/40670602-b1d0211a-63a4-11e8-9abc-a5de1f90a547.png)[^fn-2]

맨 위에 컨베이너 벨트처럼 흐르는 $$C$$값이 cell state이며, LSTM은 이 cell state를 보호하고 컨트롤 하기 위한 세 가지 게이트: forget, input, output gate를 통해 vanishing gradient를 방지하고 그래디언트가 효과적으로 흐를 수 있게 한다.

- **forget gate** $$f_t$$는 말그대로 '과거 정보를 잊기'위한 게이트다. 시그모이드 함수의 출력 범위는 0 ~ 1 이기 때문에 그 값이 0이라면 이전 상태의 정보는 잊고, 1이라면 이전 상태의 정보를 온전히 기억하게 된다.[^fn-3]
- **input gate** $$i_t$$는 '현재 정보를 기억하기'위한 게이트다. 이 값은 시그모이드 이므로 0 ~ 1 이지만 hadamard product를 하는 $$\tilde{C}_t$$는 hyperbolic tangent 결과이므로 -1 ~ 1 이 된다.[^fn-3] 따라서 결과는 음수가 될 수도 있다.
- **output gate** $$o_t$$는 최종 결과 $$h_t$$를 위한 게이트이며, cell state의 hyperbolic tangent를 hadamard product한 값이 LSTM의 최종 결과가 된다.

#### 수식 구현
수식을 살펴보고 직접 구현해보도록 한다. 여기서는 위키피디어에 등록된 [LSTM with a forget gate의 수식](https://en.wikipedia.org/wiki/Long_short-term_memory#LSTM_with_a_forget_gate)을 기준으로 구현해본다. $$x_t$$와 $$h_{t-1}$$에 서로 다른 가중치를 부여하며, $$\tilde{C}_t$$는 따로 지정하지 않고 $$c_t$$ 수식에 inline 형태로 삽입한다.

![](https://wikimedia.org/api/rest_v1/media/math/render/svg/2db2cba6a0d878e13932fa27ce6f3fb71ad99cf1)

수식을 코드로 동일하게 구현하면 아래와 같다.

```python
ft = sigmoid(np.dot(xt, Wf) + np.dot(ht_1, Uf) + bf)  # forget gate
it = sigmoid(np.dot(xt, Wi) + np.dot(ht_1, Ui) + bi)  # input gate
ot = sigmoid(np.dot(xt, Wo) + np.dot(ht_1, Uo) + bo)  # output gate
Ct = ft * Ct_1 + it * np.tanh(np.dot(xt, Wc) + np.dot(ht_1, Uc) + bc)
ht = ot * np.tanh(Ct)
```

cell state $$C_t$$와 hidden state $$h_t$$는 아래와 같이 이 전의 값을 저장해 다음 스텝에서 사용한다.

```python
ht_1 = ht  # hidden state, previous memory state
Ct_1 = Ct  # cell state, previous carry state
```

### Keras 구현
이제 직접 NumPy로 계산해 Keras의 계산 값과 비교해보도록 한다. 백엔드는 TensorFlow를 사용했다. 참고로 Keras의 LSTM 구현은 `tf.nn.rnn_cell.BasicLSTMCell`은 물론 `tf.nn.rnn_cell.RNNCell`조차 사용하지 않으며, RNN 구조를 Keras 팩키지 내에서 직접 구현하는 형태로 디자인 되어 있다.

```python
model = Sequential()
model.add(LSTM(5, input_shape=(10, 3)))

model.compile(loss='MSE',
              optimizer='SGD',
              metrics=['accuracy'])
```

LSTM 수식을 복잡하게 구현했던 것과 달리 Keras에서 LSTM을 구현하는 방법은 매우 직관적이고 쉽다. 이 점이 연구자들에게 Keras가 인기 있는 이유 중 하나다. 여기서는 레이어를 5개로 정하고 계산 결과를 비교해보도록 한다. 

#### 가중치 추출
우리가 계산하려는 NumPy 구현은 학습을 구현하지 않았기 때문에 Keras에서 학습한 가중치를 그대로 사용하도록 한다. 따라서, 가중치를 아래와 같은 형태로 추출해낸다.

```python
names = [weight.name for layer in model.layers for weight in layer.weights]
weights = model.get_weights()

for name, weight in zip(names, weights):
    print(name, weight.shape)
    print(weight)

    layer_type = name.split('/')[1]
    if layer_type == 'kernel:0':
        kernel_0 = weight
    if layer_type == 'recurrent_kernel:0':
        recurrent_kernel_0 = weight
    elif layer_type == 'bias:0':
        bias_0 = weight

    print()
```

여기서는 출력을 겸했지만 핵심은 `kernel_0`, `recurrent_kernel_0`, `bias_0` 가중치를 추출한 것이다. 가중치는 수식 코드에 적용하기 위해 변수로 따로 맵핑한다. 각각의 shape는 주석에 표기했으며, 이 학습된 가중치를 통해 디코딩 하여 결과를 맞춰볼 것이다.

```python
units = 5  # LSTM layers

# (3, 20) embedding dims, units * 4
Wi = kernel_0[:, 0:units]
Wf = kernel_0[:, units:2 * units]
Wc = kernel_0[:, 2 * units:3 * units]
Wo = kernel_0[:, 3 * units:]

# (5, 20) units, units * 4
Ui = recurrent_kernel_0[:, 0:units]
Uf = recurrent_kernel_0[:, units:2 * units]
Uc = recurrent_kernel_0[:, 2 * units:3 * units]
Uo = recurrent_kernel_0[:, 3 * units:]

# (20,) units * 4
bi = bias_0[0:units]
bf = bias_0[units:2 * units]
bc = bias_0[2 * units:3 * units]
bo = bias_0[3 * units:]
```

#### Hard Sigmoid
Keras는 특이하게도 activation으로 sigmoid 대신 hard sigmoid를 디폴트로 사용한다. 처음에 이걸 알아차리지 못해서 한동안 값을 맞추지 못했고, 결국 [SO에 질문](https://stackoverflow.com/q/49759614/3513266)을 남겼다. 거의 반나절을 고생했는데 놀랍게도 하루만에 답변이 달렸고, Keras의 LSTM activation 디폴트가 hard sigmoid 라는 점을 알아차릴 수 있었다.

```python
def hard_sigmoid(x):
    return np.clip(0.2 * x + 0.5, 0, 1)
```

그렇다면 왜 수식과 달리 hard sigmoid가 디폴트일까? 이유를 찾지는 못했지만 같은 팀 동료의 추측에 따르면 속도 개선을 위해 그랬을 것이라 추측한다. 실제로 sigmoid는 지수를 사용하는 수식이라 연산 비용이 많이 들지만 hard sigmoid는 min, max 처리에 불과해 속도가 매우 빠르면서도 선형 근사<sup>linearly approximated</sup>로 큰 차이 없이 비슷한 결과를 만들어 낼 수 있다.

이제 hard sigmoid로 수식을 다시 작성한 최종 구현은 아래와 같다.

```python
ft = hard_sigmoid(np.dot(xt, Wf) + np.dot(ht_1, Uf) + bf)  # forget gate
it = hard_sigmoid(np.dot(xt, Wi) + np.dot(ht_1, Ui) + bi)  # input gate
ot = hard_sigmoid(np.dot(xt, Wo) + np.dot(ht_1, Uo) + bo)  # output gate
Ct = ft * Ct_1 + it * np.tanh(np.dot(xt, Wc) + np.dot(ht_1, Uc) + bc)
ht = ot * np.tanh(Ct)
```

#### 계산 결과 비교
Keras 모델의 계산 결과를 출력 결과와 함께 표시하면 아래와 같다.
```python
intermediate_layer_model = Model(inputs=model.input,
                                 outputs=model.output)
output = intermediate_layer_model.predict(x[:1])
print("Keras:", output)
--
Keras: [[0.6693466  0.6416074  0.7588275  0.62952006 0.07489067]]
```

NumPy로 계산한 마지막 hidden state $$h_9$$의 출력값은 아래와 같다.
```
9 [[0.66934657 0.64160744 0.75882746 0.62952006 0.07489075]]
```

두 결과는 동일하며, 우리가 위키피디어 수식을 기준으로 구현한 NumPy 코드는 Keras 구현과 동일함을 확인할 수 있다.

## 코드
전체 코드는 [lstm-keras-inspect.py](https://github.com/likejazz/jupyter-notebooks/blob/master/deep-learning/lstm-keras-inspect.py)에서 확인할 수 있다.

## References
[^fn-1]: <http://colah.github.io/posts/2015-08-Understanding-LSTMs/>
[^fn-2]: <https://isaacchanghau.github.io/post/lstm-gru-formula/>
[^fn-3]: <https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/>