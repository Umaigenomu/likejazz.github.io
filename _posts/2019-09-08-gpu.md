---
layout: post
title: GPU 가격 및 성능 정리
tags: ["Productivity"]

---

<div class="message">
GPU 클라우드의 가격과 NVIDIA Tesla 칩셋별 성능을 정리한다.
</div>

<small>
*2019년 9월 8일 초안 작성*  
</small>

<!-- TOC -->

- [CUDA](#cuda)
- [클라우드](#클라우드)
    - [AWS](#aws)
    - [Azure](#azure)
    - [Google Cloud](#google-cloud)
    - [Best Deals in Deep Learning Cloud Providers](#best-deals-in-deep-learning-cloud-providers)
- [NVIDIA](#nvidia)
    - [T4 (Turing)](#t4-turing)
    - [V100 (Volta)](#v100-volta)
    - [P100 (Pascal)](#p100-pascal)
    - [P40, P4](#p40-p4)
    - [M60, M6 (Maxwell)](#m60-m6-maxwell)
    - [K40, K80 (Kepler)](#k40-k80-kepler)
- [기타](#기타)
    - [Low Precision Inference on GPU](#low-precision-inference-on-gpu)
    - [NumPy GPU acceleration](#numpy-gpu-acceleration)
- [References](#references)

<!-- /TOC -->

## CUDA
Developing AI applications takes many steps and each step involves processing large volumes of data and performing massive computing operations. CUDA-X AI provides the tools and technologies needed to conquer this challenge. [^fn-cuda]

[^fn-cuda]: <https://www.nvidia.com/en-us/technologies/cuda-x/>

<img src="https://www.nvidia.com/content/dam/en-zz/Solutions/technologies/cuda-x/cuda-x-ai-ecosystem-diagram-1ccw-l.png">

## 클라우드
### AWS

| Type | On Demand | RI(3yrs) |
| ---- | ------------- | -------- |
| P3/V100 | $4.234 | $1.676(60%) |
| G3/M60 | $1.420 | $0.685(52%) |
| P2/K80 | $1.460 | $0.679(54%) |

### Azure
서울 리전은 V100만 지원한다.

### Google Cloud
GCP는 아직 한국 리전이 없다.

| Region | Type | On Demand | Preemptible | 1yr | 3yrs | Latency |
| ------ | ---- | --------- | ----------- | --- | ---- | ------- |
| 도쿄(asia-northeast1) | T4 | $1.03 | $0.31 | $0.65 | $0.47 | 85.963782ms |
| 타이완(asia-east1) | V100 | $2.48 | $0.74 | $1.562 | $1.116 | 145.619823ms |
| 타이완(asia-east1) | P100 | $1.60 | $0.43 | $1.008 | $0.720 | |
| 타이완(asia-east1) | K80 | $0.49 | $0.135 | $0.308 | N/A | |
| Oregon(us-west1) | K80 | | | $206.590 (monthly)<br>$0.198 (effective hourly rate) | | 245.904412ms |

<img src="https://user-images.githubusercontent.com/1250095/64484263-97573b00-d24a-11e9-96e4-853574766863.png">

- A 30% discount off the cost of using 1 GPU for a full month.  
- K80 정가(타이완, 한 달 정가 적용): $224.840 272,250 KRW
- K80 최저가(Oregon, 월 30% 할인 적용): $144.613 **172,740 KRW**

### Best Deals in Deep Learning Cloud Providers
<img src="https://miro.medium.com/max/2696/1*ooV7tfwd5sJeSazuMGYYJg.png" width="80%">[^fn-deal]

[^fn-deal]: <https://towardsdatascience.com/maximize-your-gpu-dollars-a9133f4e546a>

## NVIDIA

GPGPU 제품군인 NVIDIA Tesla 칩셋을 정리[^fn-tesla]한다.

- NVIDIA Quadro cards are designed for accelerating CAD.
- NVIDIA Tesla cards are designed for scientific computations.
- NVIDIA RTX is a graphics rendering development platform.

### T4 (Turing)
- PCIe: 2560 * 585/1590 MHz, 8100 GFLOS, 16G (Sep 2018)

> 차세대 인퍼런스 가속을 위해 특별히 개발되었습니다. The World's Most Advanced Inference Accelerator. NVIDIA Turing Tensor Core를 기반으로 AI 인퍼런스 속도를 CPU 대비 40배 이상 빠른 속도로 가속합니다. [^fn-nvidia-desc]

### V100 (Volta)
- mezzanine(NVLink): 5120 * ?/1455 MHz, 14899 GFLOPS, 16G or 32G (Jun 2017)

### P100 (Pascal)
- mezzanine(NVLink): 3584 * 1328/1480 MHz, 9519–10609 GFLOPS, 16G (Apr 2016)

> 딥러닝을 위한 제품으로 Mezzanine과 PCI-Express 모델로 구분되며 3,584개의 CUDA 코어를 제공합니다.

### P40, P4
- P40: 3840 * 1303/1531 MHz, 10007–11758 GFLOPS, 24G (Sep 2016)
- P4: 2560 * 810/1063 MHz, 4147–5443 GFLOPS, 8G (Sep 2016)

> 인퍼런스(Inferences)에 특화된 카드로 각각 3,584/1,024 CUDA 코어를 제공합니다.

### M60, M6 (Maxwell)
- M60: 4096 * 899/1178 MHz, 7365–9650 GFLOPS, 2x8G (Aug 2015)
- M6: 1536 * 722/1051 MHz, 2218–3229 GFLOPS, 8G (Aug 2015)

> 엔터프라이즈에서 사용하는 서버용 모델입니다. M60은 랙 타입 서버에 탑재되고, M6는 블레이드 서버에 장착됩니다. M60은 4,096개의 CUDA 코어를 제공하며, 16GB의 GDDR5 메모리가 제공됩니다. M6는 1,536 CUDA 코어, 8GB GDDR5 메모리를 주요 사양으로 합니다.

### K40, K80 (Kepler)
- K80: 4992 * 560/875 MHz, 5591–8736 GFLOPS, 2x12G (Nov 2014)
- K40: 2880 * 745/875 MHz, 4291–5040 GFLOPS, 12G (Oct 2013)

[^fn-nvidia-desc]: <https://www.udna.kr/tesla?gclid=Cj0KCQjw5MLrBRClARIsAPG0WGx1nHcumfgpb3FMT-4OcMA2V6S-gQNL9GT4EzAjyUvoEEqdk6L8GJsaAhchEALw_wcB>
[^fn-tesla]: <https://en.wikipedia.org/wiki/Nvidia_Tesla>

## 기타
### Low Precision Inference on GPU
<img src="https://user-images.githubusercontent.com/1250095/64407402-2e8f8780-d0bf-11e9-8eb9-ff3aa8ceb348.jpg" width="80%">

T4 GPU에서 Quantized 연산으로 8.1TFLOPS를 130 TOPS, 16배 개선할 수 있다. 그러나, BERT SQuAD 1.1(F1) 점수가 Int8에서 6.43%나 낮다. Activation으로 GeLU10을 사용해 0.67% 이내로 줄일 수 있다. [^fn-pdf] MRPC는 두 문장의 유사 문장 판별 태스크로 마찬가지로 성능 차이를 0.80% 이내로 줄일 수 있다.

[^fn-pdf]: [LOW PRECISION INFERENCE ON GPU(PDF)](https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9659-inference-at-reduced-precision-on-gpus.pdf) 

<img src="https://user-images.githubusercontent.com/1250095/64407574-9c3bb380-d0bf-11e9-908c-83783fcb6692.jpg" width="80%">

### NumPy GPU acceleration
[^fn-numpy]  
- **CuPy**, which has a NumPy interface for arrays allocated on the GPU used in Chainer.
- **Numba** is an JIT compiler using the industry-standard LLVM compiler library that translates a subset of Python and NumPy code into fast machine code.
- **PyCUDA** lets you access Nvidia's CUDA parallel computation API from Python. 
- **PyTorch** as you know very well.
- **JAX** can automatically differentiate native Python and NumPy functions with updated version of Autograd.

[^fn-numpy]: <https://stsievert.com/blog/2016/07/01/numpy-gpu/>

## References