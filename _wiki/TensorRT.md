---
layout: wiki 
title: TensorRT
---

<!-- TOC -->

- [TensorRT & Inferences](#tensorrt--inferences)
- [Tutorial](#tutorial)
- [Training](#training)
- [NLP w/ BERT](#nlp-w-bert)

<!-- /TOC -->

당분간 링크만 정리한다.

# TensorRT & Inferences
- [TensorRT Integration Speeds Up TensorFlow Inference](https://devblogs.nvidia.com/tensorrt-integration-speeds-tensorflow-inference/)  
TensorFlow v1.7 and above integrates with TensorRT 3.0.4. 
- [TensorRT를 활용한 딥러닝 Inference 최적화](https://www.slideshare.net/deview/232-dl-inference-optimization-using-tensor-rt-1-119162975)
- [NVIDIA TensorRT Inference Server Boosts Deep Learning Inference](https://devblogs.nvidia.com/nvidia-serves-deep-learning-inference/)
- [How to Speed Up Deep Learning Inference Using TensorRT](https://devblogs.nvidia.com/speed-up-inference-tensorrt/)

# Tutorial
- [Object Detection on GPUs in 10 Minutes](https://devblogs.nvidia.com/object-detection-gpus-10-minutes/)

# Training
- [Video Series: Mixed-Precision Training Techniques Using Tensor Cores for Deep Learning](https://devblogs.nvidia.com/video-mixed-precision-techniques-tensor-cores-deep-learning/)
- [Mixed-Precision Training of Deep Neural Networks](https://devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/)

# NLP w/ BERT
- [Real-Time Natural Language Understanding with BERT Using TensorRT](https://devblogs.nvidia.com/nlu-with-tensorrt-bert/)
- [ADDING A CUSTOM CUDA C++ OPERATIONS IN TENSORFLOW FOR BOOSTING BERT INFERENCE](https://on-demand-gtc.gputechconf.com/gtcnew/sessionview.php?sessionName=skr9108-adding+a+custom+cuda+c%2b%2b+operations+in+tensorflow+for+boosting+bert+inference)