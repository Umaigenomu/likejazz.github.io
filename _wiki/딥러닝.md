---
layout: wiki 
title: 딥러닝
---

<!-- TOC -->

- [개요](#개요)
- [경사 하강법<sup>Gradient Descent<sup>](#경사-하강법gradient-descent)
- [성능](#성능)
- [용어 정리](#용어-정리)
- [미분](#미분)
- [최적화<sup>Optimization</sup>](#최적화optimization)
    - [GD vs. SGD](#gd-vs-sgd)
    - [AdaGrad](#adagrad)
    - [최적화 발달 계보](#최적화-발달-계보)
- [기울기<sup>Gradient</sup>](#기울기gradient)
    - [기울기 소실<sup>Gradient Vanishing</sup>](#기울기-소실gradient-vanishing)
- [배치 정규화<sup>Batch Nomarlization</sup>](#배치-정규화batch-nomarlization)
- [가중치 감소<sup>Weight Decay</sup>](#가중치-감소weight-decay)

<!-- /TOC -->

# 개요
프로그래머들이 명시적인 규칙을 충분하게 많이 만들어 지식을 다루면 인간 수준의 인공 지능을 만들 수 있다고 믿었습니다. 이런 접근 방법을 심볼릭 AI <sup>symbolic AI</sup>라고 하며 1950년대부터 1980년대까지 AI 분야의 지배적인 패러다임이었습니다. 1980년대 심볼릭 AI의 새로운 버전인 전문가 시스템 <sup>expert system</sup>이 큰 기업들 사이에서 인기를 끌기 시작했습니다. (케라스 창시자에게 배우는 딥러닝, 2017)

초창기에 인공지능 학계는 두 진영으로 분열되었다. 한쪽은 이른바 규칙 기반 또는 상징적 인공지능을 추구했다. 다른 한쪽은 통계적 패턴 인식 시스템을 구축했다. 전자는 성인이 외국어를 배우려고 노력하는 방식으로 인공지능에 접근하려고 시도했다. 후자는 아이가 첫 언어를 배우는 것과 흡사한 방식으로 인공지능을 구축하려고 시도했다. (머신 플랫폼 크라우드, 2017)

# 경사 하강법<sup>Gradient Descent<sup>
<img src="https://user-images.githubusercontent.com/1250095/36552767-041f2a9a-183e-11e8-9443-6582c101c4b3.png" width="70%" />

수식 <img src="http://chart.apis.google.com/chart?cht=tx&chl=f(x)=x^4-3x^3%2B2" />은 상기 미분 그래프를 보면 0에 local minima, 2.25에 global minima가 있다. ([위키피디어](https://ko.wikipedia.org/wiki/%EA%B2%BD%EC%82%AC_%ED%95%98%EA%B0%95%EB%B2%95)) 경사 하강법으로 minima를 찾는 알고리즘은 아래와 같다.

```
x_old = 0
x_new = 6
eps = 0.01
precision = 1e-5
step = 1

def f_prime(x):
    return 4*x**3-9*x**2

while abs(x_new - x_old) > precision:
    x_old = x_new
    x_new = x_old - eps * f_prime(x_old)
    print('#%d x_old:%f, x_new:%f, grad:%f, diff:%.8f' % (step, x_old, x_new, f_prime(x_old), (x_new - x_old)))
    step += 1

print("Minimum occurs at %f" % x_new)
```

약 70여회 반복 후 2.25를 찾아낸다.

```
...
#67 x_old:2.249913, x_new:2.249930, grad:-0.001772, diff:0.00001772
#68 x_old:2.249930, x_new:2.249944, grad:-0.001413, diff:0.00001413
#69 x_old:2.249944, x_new:2.249956, grad:-0.001127, diff:0.00001127
#70 x_old:2.249956, x_new:2.249965, grad:-0.000899, diff:0.00000899
Minimum occurs at 2.249965
```

물론 이 경우 초기값이 0보다 작다면 local minima인 0에 빠지게 된다.

# 성능
GPU 클라우드 서버(모델명 Tesla P40)에서 `tf.device`로 CPU를 강제 할당 할 경우 맥북 프로에 비해서 1/2 성능 밖에 나오지 않았다. 실제로 서버의 CPU 클럭이 맥북 프로에 비해 떨어지며(2.6 GHz vs 2.2 GHz) 무엇보다 실수로 4코어로 생성했기 때문이다. 맥북 프로는 8코어 이며, 서버에도 마찬가지로 8코어로 했을때 1.5배 정도 성능 개선이 있었다. 즉, 맥북 프로에서 25s가 걸리는 행렬 연산이 서버에서 32s에 진행됐다. GPU를 사용했을때는 14s에 끝났는데, GPU를 2장으로 지정해도 동일하다. 이는 단순 행렬이기 때문으로 보이며, 여러 채널로 레이어를 확장하여 채널별로 GPU를 할당하면 충분한 성능 개선이 이뤄질 것으로 보인다.

행렬 크기가 너무 작아 256 중간 노드를 4096으로 키웠는데, 맥북 프로에서 126s 걸리던 작업이 GPU에서 3.8s로 무려 30배 이상의 성능 개선이 있다. 앞서는 행렬 사이즈가 작아서 별다른 차이점이 없었던 것이며, 이처럼 행렬이 충분히 크면 30배 이상 개선된 속도를 보여준다. 단, 단일 행렬에서 2 GPU와 1 GPU의 속도 차이는 없었고 실제로 `nvidia-smi`에서 하나의 GPU만 동작하는 모습을 확인할 수 있다.

| 타입 | 노드 | epoch | 성능 |
| --- | --- | --- | --- |
| 맥북 프로 | 256 | 5000 | 20s |
| | 4096 | 5000 | 239s |
| Tesla P40 x 1 | 256 | 5000 |  8.5s |
| | 4096 | 5000 | 11.26s |
| Tesla P40 x 2 | 256 | 5000 |  8.59s |
| | 4096 | 5000 |  11.33s |


# 용어 정리
[TeX 표기법이 잘 정리](https://docs.moodle.org/23/en/Using_TeX_Notation_2)되어 있다.  
역자가 『밑바닥부터 시작하는 딥러닝』 을 번역하며 [딥러닝 용어표](https://docs.google.com/spreadsheets/d/1ccwGiC01X-gs3PPcXPUz67W9rS6l994LD4AL18KF1_0/edit#gid=0)를 정리했다.

# 미분
<img src="http://docs.likejazz.com/images/2017/derivatives.png" width="400" />  

(Numerical Methods for Engineers, 2009)

(a)전진차분, (b)후진차분, (c)중앙차분<sup>Centered Finite-Divided-Difference</sup>의 표현 비교. [중앙차분이 실제미분과 가장 가까운 것](http://blog.naver.com/mykepzzang/220072089756)을 확인할 수 있다.

<img src="http://chart.apis.google.com/chart?cht=tx&chl=f^{'}(x){\approx}{\frac{f(x%2Bh)-f(x-h)}{2h}}" />

수치 미분의 h 즉, <img src="http://chart.apis.google.com/chart?cht=tx&chl={\delta}" /> 값은 0.0001(1e-4)로 지정. 범위가 클수록 해석적 미분의 값과 차이가 난다. cs231n의 만족스러운 기준은 1e-7이라고.

# 최적화<sup>Optimization</sup>

## GD vs. SGD

[Sebastian Raschka의 답변](https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent)에 따르면, GD는 1 epoch 동안 모든 트레이닝셋에 대한 gradients의 sum이다. 수식이 mean이 아니라 sum인데, 생각해보면 어차피 음수도 나오고 양수도 나오기 때문에 sum으로 인한 문제가 없다. 배치 단위로 업데이트 하기 때문에 batch GD라고도 불린다.

SGD는 샘플 단위로 gradients를 업데이트 한다. stochastic이라는 용어는 통계학에서 쓰이는 확률론적 근사 <sup>stochastic approximation</sup>에서 따왔다고 한다. 이런 '확률론적' 특성 때문에 GD 처럼 global minima에 직접 나아가진 않지만 지그재그 형태로 접근하게 되며, cost function이 convex 형태라면 SGD 또한 충분히 global minima에 잘 수렴한다. (Bottou, Léon, 1998)

그리고 several different flavors of SGD로 shuffling에 대해 언급한다. 즉, 랜덤 샘플링은 일종의 SGD의 변종(variants)인 셈이다. 세가지 경우를 언급하는데 요점은 같다. 섞어주고 매 번 다른 데이터 batch가 들어가게 한다. TensorFlow에도 `next_batch()` 메소드는 `shuffle=True`가 디폴트 옵션으로 되어 있으며 매 배치시 데이터가 섞여서 유입된다.

MINI-BATCH는 일정 개수의 샘플을 묶어서 사용한다. 일반적인 미니 배치 사이즈는 k=50이며, 50 묶음 트레이닝 샘플 단위로 gradients 업데이트를 한다.

<img src="http://docs.likejazz.com/images/2017/w-sgd.png" width="80%" />

SGD로 학습할때 w_1950(임의의 가중치)의 값 변화 그래프

GD 기반의 [다양한 최적화 알고리즘 소개](http://sebastianruder.com/optimizing-gradient-descent/)를 정리한 글이다.

## AdaGrad
학습률을 자동적으로 결정하는(변형하는) 방법으로 가장 많이 사용된다. (딥러닝 제대로 시작하기, 2015) 이전 기울기의 제곱합의 제곱근을 분모로 하여 학습률을 점점 낮춘다.

<img src="http://www.logos.t.u-tokyo.ac.jp/~hassy/deep_learning/adagrad/adagrad.png" width="30%" />

## 최적화 발달 계보
<img src="https://image.slidesharecdn.com/random-170910154045/95/-49-638.jpg?cb=1505089848" />

[출처](https://www.slideshare.net/yongho/ss-79607172)

# 기울기<sup>Gradient</sup>
경사 하강법의 기울기 업데이트 수식은 아래와 같다.  
<img src="http://chart.apis.google.com/chart?cht=tx&chl=w_i\leftarrow%20w_i-\eta\frac{\partial%20E}{\partial%20w_i}" />

delta_w 수식은 [Delta rule](https://en.wikipedia.org/wiki/Delta_rule)로 정리되어 있다.  
<img src="http://chart.apis.google.com/chart?cht=tx&chl={\Delta}w_i=-\eta\frac{\partial%20E}{\partial%20w_i}," />

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cde1e8dbb829eac83cd093e6cedfa2298e574927" />

Delta rule에서 구하기 가장 까다로운 부분은 y_j 즉, actual output이며, 이유는 이전 레이어로 깊이 내려갈수록 편미분을 하기 위해 수식을 계속 풀어헤쳐야 해서 점점 더 복잡해지기 때문이다. 행렬 연산을 이용하면 출력 레이어 부터 꾸준히 에러를 누적해 나갈 수 있으며, 이 부분이 역전파의 핵심이라고 생각한다. 기본적으로 구하고자 하는 가중치의 출력 노드 다음 노드의 activation 미분과 이전 가중치를 꾸준히 곱해 나가면 된다.

## 기울기 소실<sup>Gradient Vanishing</sup>
신경망이 깊을때 활성화 함수를 통한 기울기 값 0 ~ 1은 매우 작은 값이고 역전파<sup>Backpropagation</sup>중에 배가되어 깊은 망에서 소실<sup>Vanishing</sup>되는 효과를 가져온다. ReLU와 LSTM 아키텍처를 사용하여 해결하는 방법이 일반적이다.

# 배치 정규화<sup>Batch Nomarlization</sup>
- 학습을 빨리 진행할 수 있다.
- 초깃값에 크게 의존하지 않는다.
- 오버피팅을 억제한다.

[Batch Normalization과 Layer Normalization 소개](https://theneuralperspective.com/2016/10/27/gradient-topics/)

# 가중치 감소<sup>Weight Decay</sup>
오버피팅을 줄이기 위해 큰 가중치에 대해 L2 norm을 이용한 페널티를 준다.

<img src="http://chart.apis.google.com/chart?cht=tx&chl=\widetilde{E}(\mathbf{W})=E(\mathbf{W})%2B\frac{\lambda}{2}\mathbf{W}^2" />  

에러 펑션에 L2 정규화<sup>L2 Regularization</sup> <img src="http://chart.apis.google.com/chart?cht=tx&chl=W={\sqrt{W_1^2%2BW_2^2%2B...%2BW_n^2}}" />를 추가 했다. (밑바닥부터 시작하는 딥러닝, 2016)

경사 하강법에 가중치 감소를 적용한 새로운 비용 함수:[14](http://stats.stackexchange.com/a/31334)  
(개별 가중치 w_i를 업데이트 할 경우에 대한 수식)

<img src="http://chart.apis.google.com/chart?cht=tx&chl=w_i{\leftarrow}w_i-\eta\frac{\partial%20E}{\partial%20w_i}-\eta\lambda%20w_i" />

큰 가중치에 페널티를 주고 효과적으로 모델을 제한한다.

- <img src="http://chart.apis.google.com/chart?cht=tx&chl={\eta}" />: 학습률<sup>learning rate</sup>  
- <img src="http://chart.apis.google.com/chart?cht=tx&chl={\lambda}" />: 정규화 파라미터

overfitting을 피하는 처리 과정을 규제 <sup>regularization</sup>라고 한다. L2 regularization의 경우 동일한 파라미터 수를 갖고 있더라도 사용한 모델이 기본 모델 보다 훨씬 더 overfitting에 잘 견딘다. val loss가 반대 방향으로 거의 증가하지 않음. (케라스 창시자에게 배우는 딥러닝, 2017)
