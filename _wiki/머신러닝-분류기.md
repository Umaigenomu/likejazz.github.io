---
layout: wiki 
title: 머신러닝 분류기
---

<!-- TOC -->

- [Regression Analysis](#regression-analysis)
- [Decision Trees](#decision-trees)
    - [CART](#cart)
    - [ID3](#id3)
- [Random Forest](#random-forest)
- [Gradient Boosting](#gradient-boosting)
- [k-Means Clustering](#k-means-clustering)
    - [scree plot](#scree-plot)
- [k-Nearest Neighbors(KNN)](#k-nearest-neighborsknn)

<!-- /TOC -->

# Regression Analysis
선형 회귀와 로지스틱 회귀는 같은 기본 수식을 사용하지만 선형은 연속된 값 Y를 찾는 반면, 로지스틱은 분류 결과를 찾는다. 즉, 0 또는 1의 값을 도출한다.

그림으로 잘 설명한 [선형](http://adit.io/posts/2016-02-20-Linear-Regression-in-Pictures.html)/[로지스틱](http://adit.io/posts/2016-03-13-Logistic-Regression.html) 회귀, 이 사람의 그림으로 쉬운 설명에 감동해 책 『Grokking Algorithms』 도 구매해서 읽었다.

로지스틱 회귀는 전통적으로 two-class classification에 제한적이다. 분류가 2 종류를 초과할 경우 LDA<sup>Linear Discriminant Analysis</sup> 알고리즘이 주로 선호되는 linear classification technique이다.

# Decision Trees
노드에 포함된 모든 example들이 원하는 y값에 대해 같은 y값을 가지고 있을때 pure하다. 아래는 [타이타닉 데이타  분석 결과](https://nbviewer.jupyter.org/github/likejazz/jupyter-notebooks/blob/master/machine-learning/titanic.ipynb). dtreeviz가 보다 직관적이다.

<img src="https://raw.githubusercontent.com/likejazz/jupyter-notebooks/master/machine-learning/data/titanic.png" width="100%" />

<img src="https://user-images.githubusercontent.com/1250095/55542969-0f04c500-5703-11e9-860f-749b987d33c1.png" width="100%">

decision trees는 80.9%, random forest는 81.6%. 데이타의 변별력이 부족하므로 큰 차이는 없으나, [전처리를 통해 92%까지](https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8) 높일 수 있었다.

## CART
CART 알고리즘에서는(scikit-learn은 CART를 최적화한 알고리즘 사용) Gini impurity를 디폴트로 사용한다. *Not to be confused with Gini coefficient.* CART는 greedy 알고리즘이다.  

$$ \textit{Gini}: \mathit{Gini}(E) = 1 - \sum_{j=1}^{c}p_j^2 $$

$$ \textit{Entropy}: H(E) = -\sum_{j=1}^{c}p_j\log p_j $$

수식대로 계산해보면 균등하게 분류될수록 Gini impurity가 높다. 한쪽으로 쏠릴 수록 Gini impurity가 낮다. e.g. {0.5, 0.5} = 0.5, {0.1, 0.9} = 0.18. *A perfect separation results in a Gini score of 0.* 기본적으로 Gini impurity와 Information gain entropy는 거의 동일하다.

## ID3
ID3 (Iterative Dichotomiser 3) is an algorithm invented by Ross Quinlan(1986) (Wikipedia)

ID3, C4.5에서는 Entropy를 기준으로 하는 Information gain을 사용한다.
> information gain is a synonym for Kullback–Leibler divergence (Wikipedia)

<img src="https://cdn-images-1.medium.com/max/1600/1*bVGWGETTor7bSnhr7sXEVw.png"> ([source](https://medium.com/coinmonks/what-is-entropy-and-why-information-gain-is-matter-4e85d46d2f01))


<img src="https://cdn-images-1.medium.com/max/1200/0*wRIiE6JizPZKiRlE"> [계산](https://medium.com/@rakendd/building-decision-trees-and-its-math-711862eea1c0), [코드 구현](https://nbviewer.jupyter.org/github/likejazz/jupyter-notebooks/blob/master/machine-learning/decision-tree-from-scratch.ipynb)

# Random Forest
> 넷플릭스 프라이즈의 우승자는 머신러닝 알고리즘 수백 가지를 통합한 메타학습 알고리즘을 사용했다. 왓슨은 메타학습 알고리즘을 사용해 후보로 올라온 대답들 중에서 최종 대답을 선택했다. 이런 종류의 메타학습을 스택킹 <sup>stacking</sup>이라 부르며 '세상에 공짜는 없다'라는 정리의 창시자 데이비드 월퍼트가 생각해낸 방식이다. 이보다 훨씬 더 간단한 메타학습은 통계학자 레오 브라이먼<sup>Leo Breiman et al., 2001</sup>이 발명한 배깅 <sup>bagging</sup>이 있다. 배깅은 여러 학습 예제를 무작위로 만들어 알고리즘에 적용 후 그 결과들을 투표 방식으로 통합한다. 이렇게 하는 까닭은 분산이 줄어들기 때문이다. 배깅은 데이터가 예상 밖의 변화가 생겼을때 단일 모델보다 훨씬 덜 민감하므로 정확도를 향상하는 매우 쉬운 방법이다. 키넥트는 랜덤 포레스트를 사용한다.  
p.384 『마스터 알고리즘』 <sub>2015, 2016</sub>

- bagging trees
  - b=1...B random sampling
  - tree 구성(ID3)
  - classification B개 모든 tree를 사용해서 분류, majority vote로 결정.
  - decrease variance while bias stays same.
- attributes random

특징은 아래와 같다.
- easy to interpret. 『수학 없이 배우는 데이터 과학과 알고리즘』 에서는 '블랙박스' 모델이라고 언급했으나 어렵게나마 도식화가 가능하다는 점에서 딥러닝 처럼 완전한 블랙박스로 보긴 힘들다. 다만, decision trees에 비해서는 해석하기 어렵다.
- can induce non-linear decision boundaries.
- fast at prediction(O(height of tree))

scikit-learn의 random forest에서 `tree.tree_.threshold[x]`의 값을 직접 수정하여 decision rule을 변경해서 원본 decision tree(상단), decision tree의 rule을 수정(하단)한 것과 decision boundaries의 변화를 도식화한 모습이다.

<img src="http://docs.likejazz.com/images/2017/iris-rf-decision-boundaries.png" width="60%" />

# Gradient Boosting
> 학습이론가 요아브 프로인트 <sup>Yoav Freund</sup>와 롭 샤피르 <sup>Rob Schapire</sup>가 발명한 부스팅 <sup>boosting</sup>이 있다. 부스팅은 여러 학습 알고리즘을 결합하는 대신 이전 모형들이 저지른 실수를 바로잡는 새 모형을 이용하면서 같은 분류기를 데이터에 반복 적용한다. 학습을 할 때마다 잘못 분류한 사례의 가중치를 증가시켜 다음번 학습에서는 이 사례에 더욱 집중하도록 한다. 부스팅이라는 이름은 이 과정이 처음에는 무작위 추측보다 그저 약간 좋기만 한 분류기를 지속적으로 강화하여 거의 완전한 분류기로 만든다는 개념에서 나왔다.  
p.384 『마스터 알고리즘』 <sub>2015, 2016</sub>

이전 예측기가 만든 잔여 오차<sup>residual error</sup>에 새로운 예측기를 학습시킨다. Decision Trees를 기반 예측기로 하는 회귀 문제 풀이를 gradient tree boosting 또는 gradient boosted regression tree(GBRT)라고 한다.

# k-Means Clustering
<img src="https://user-images.githubusercontent.com/1250095/47555932-164fc780-d948-11e8-9e0d-219e11fbff62.jpeg" width="70%" />  

(통계학 도감, 2017)

## scree plot
<img src="https://upload.wikimedia.org/wikipedia/commons/a/ac/Screeplotr.png" />
(Wikipedia)

적당한 군집 갯수를 결정하기 위해 스크리 플롯을 사용한다. 급격히 구부러지는 지점 kinks가 최적의 군집 갯수다. k-Means 또는 주성분 갯수를 구하기 위한 PCA에서도 사용한다.

# k-Nearest Neighbors(KNN)
k개의 최근접 이웃 사이에서 가장 많은 항목으로 분류. '가까움'은 유클리드 거리<sup>Euclidean distance</sup> 주로 사용. scikit-learn에서는 분류/회귀 모두 제공. 복잡한 벡터에서 k가 클수록 오버피팅이 줄어들어 부드러운 경계가 생긴다.  
<img width="70%" src="http://bdewilde.github.io/assets/images/2012-10-26-knn-example-ks.png">  
(Classification of Hand-written Digits)
