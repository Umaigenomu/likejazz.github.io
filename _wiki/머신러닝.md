---
layout: wiki 
title: 머신러닝
---

<!-- TOC -->

- [개요](#개요)
- [데이터 과학](#데이터-과학)
- [차원 축소<sup>Dimensionality Reduction</sup>: PCA](#차원-축소dimensionality-reduction-pca)
- [Markov Chains](#markov-chains)
- [Grid Search](#grid-search)
- [Scaling, Normalization, Standardization](#scaling-normalization-standardization)
- [정보 이론](#정보-이론)
- [단순성의 원리](#단순성의-원리)
- [Books](#books)
    - [Algorithms of the Intelligent Web <sub>2009</sub>](#algorithms-of-the-intelligent-web-2009)
    - [마스터 알고리즘 <sub>2015, 2016</sub>](#마스터-알고리즘-2015-2016)

<!-- /TOC -->

# 개요
<img src="https://developer.nvidia.com/sites/default/files/Deep_Learning_Icons_R5_PNG.jpg.png" width="80%">

<img src="https://cdn-images-1.medium.com/max/1600/0*a2ER5Ki6Px0rFXCm.png" width="80%">[^fn-data]

[^fn-data]: <https://towardsdatascience.com/deep-misconceptions-about-deep-learning-f26c41faceec>

# 데이터 과학
<img width="50%" src="https://user-images.githubusercontent.com/1250095/33862806-e634de66-df27-11e7-8974-d5fa5cbb2652.png">

(Principles of Data Science, 2016)

책에 등장한 벤 다이어그램이 인상적이다. **Danger Zone** 표시가 재밌다. 수학 & 통계학 지식이 없다면 위험하다는 것.

# 차원 축소<sup>Dimensionality Reduction</sup>: PCA
PCA는 회전된 특징이 통계적으로 상관 관계가 없도록 데이타셋을 회전시키는 방법이다. 상관도가 높은 변수를 통합한다는 점에서 차원 축소<sup>dimensionality reduction</sup> 기법이라 한다. feature를 선별하는 것과 함께 feature engineering(extraction) 범주에 포함된다.

`PCA(whiten=True)`: This is the same as using `StandardScaler` after the transformation. whitening corresponds to not only rotating the data, but also **rescaling** it.

속성 추출 기법
- 선형 PCA<sup>principal component analysis</sup> 선형 제한
- 비선형 MDS<sup>multidimensional scaling</sup>

PCA 주로 성분 분석
1. 데이터에서 평균값 빼서 데이터를 중심에
1. 공분산<sup>covariance</sup> 매트릭스 계산
1. 공분산의 고유벡터<sup>eigenvector</sup> 계산

# Markov Chains  
- [시각화 제공](http://setosa.io/ev/markov-chains/) 이외에도 여러 시각 자료가 있는데 멋진 구현
<img src="http://docs.likejazz.com/images/2017/markov-chains.png" width="50%" />  

마코프 체인의 결과는 일정 비율로 수렴한다.

# Grid Search
하이퍼 파라미터를 결정하기 위해 다양한 파라미터로 실험을 진행해 최적의 파라미터를 찾는 과정이다. [scikit-learn의 Grid Search 문서](http://scikit-learn.org/stable/modules/grid_search.html)에 잘 정리되어 있다.

# Scaling, Normalization, Standardization
- Scaling: 서로 다른 단위의 데이터를 같은 단위로 만들어서 큰 숫자가 더 중요해보이는 왜곡을 막는 것
- Standardization(= z-score normalization): 분포를 평균 0, 표준편차 1로 바꾸는것
- Normalization(= Min-Max scaling) : 변수를 0과 1사이로 바꾸는것

스케일링을 위한 노말라이제이션을 스탠다더라이제이션으로 할 수 있어서 구분이 어렵습니다. ([출처](https://www.facebook.com/groups/TensorFlowKR/permalink/798631867144540/?comment_id=798637187144008&comment_tracking=%7B%22tn%22%3A%22R%22%7D))

# 정보 이론
정보량은 불확실성과 직접적 관련이 있다. 매우 불확실한 일이나 전혀 모르는 일을 이해하려면 많은 정보를 파악해야 한다. 반대로 이미 많이 알고 있는 일은 정보가 많지 않아도 쉽게 이해할 수 있다. 이런 점에서 정보량은 불확실성의 크기와 같다고 볼 수 있다.

> (Information) Entropy, in other words, is a measure of uncertainty. ([source](https://towardsdatascience.com/entropy-is-a-measure-of-uncertainty-e2c000301c2c))

1948년 클로드 섀넌이 그의 유명한 논문 "A Mathematics Theory of Communication"에 '정보 엔트로피' 개념을 제기하면서 정보의 단위 문제가 풀렸고, 정보의 역할을 계량화 할 수 있게 되었다. (수학의 아름다움, 2014, 2019)

월드컵 우승팀을 가리는 문제에서 브라질 처럼 우승 확률이 매우 높은 국가에 대한 사전 정보가 있다면 그 쪽을 선택하여 엔트로피를 더욱 낮출 수 있다. (수학의 아름다움에서 정리 필요)

# 단순성의 원리
* 뉴턴의 제1법칙: 관성의 법칙  
물체의 질량 중심은 외부 힘이 작용하지 않는 한 일정한 속도로 움직인다.  
우쥔은 이를 단순성의 원칙이라 표현했다.

* 오컴의 면도날(Occam's Razor 또는 Ockham's Razor)  
> simpler solutions are more likely to be correct than complex ones  
단순한 솔루션이 복잡한 솔루션 보다 정확할 가능성이 높다.

# Books
## Algorithms of the Intelligent Web <sub>2009</sub>
『Algorithms of the Intelligent Web』 챕터 1의 전체 노트 중 몇 가지만 따로 정리한다.
- Inference does not happen instantaneously. 추론은 순식간에 되지 않는다. 모든 데이타셋/알고리즘이 응답 시간 제한내에서 실행될 것이라고 가정하면 안된다.
- Size matters!
- Different algorithms have different scaling characteristics. Some algorithms are scalable, and **others aren’t.** 수십억 개의 뉴스 중에서 유사한 제목의 뉴스 그룹을 찾는다고 가정해보자. Not all clustering algorithms can run in parallel. 경우에 따라서는 데이타 자체를 분할해야 할 수 있다.
- Everything is not a nail! 모든 문제에 통용되는 만능 알고리즘은 없다. 이 책의 저자는 Pedro Domingos의 2012년 ACM 논문을 참고 했고, 그는 『Master Algorithms』 이라는 만능 알고리즘이 존재할 것이라는 책을 쓰긴 했지만.
- Human intuition is problematic. 직관하기 어려운 고차원에서 다변량 가우스 분포<sup>multivariate Gaussian distribution</sup>의 대부분은 평균 근처에 있지 않고 바깥<sup>shell</sup>에 있다. 낮은 차원에서 간단한 분류자를 만드는 것은 쉽지만 차원 수를 늘리면 어떤 일이 일어나는지 이해하기 어렵다.
- "garbage in, garbage out."
- Learn many different models. 넷플릭스 프라이즈의 우승자는 100개 이상의 학습기를 앙상블로 구축했다고 한다.

(Algorithms of the Intelligent Web, 2016) - *[Chapter 2](https://www.safaribooksonline.com/library/view/allgorithms-of-the/9781617292583/kindle_split_010.html)부터 다시 읽어야 한다, 2017년 12월*

## 마스터 알고리즘 <sub>2015, 2016</sub>
머신러닝 다섯 종족  
<img src="https://sebastianraschka.com/images/faq/classifier_categories/master_chart.jpg" width="47%" style="padding-right: 10px; float: left" /><img src="https://user-images.githubusercontent.com/1250095/57499850-13cd2200-731c-11e9-8897-b9432f0a5ede.jpeg" width="47%" />

<p style="clear:both"></p>
왼쪽 [Sebastian Raschka의 정리](https://sebastianraschka.com/faq/docs/classifier-categories.html#pedro-domingos-5-tribes-of-machine-learning)와 오른쪽은 내가 직접 책에서 촬영했다.

합리주의자 <sup>The rationalist</sup>는 첫 행동을 개시하기 전에 모든 것을 계획한다. 경험주의자 <sup>The empiricist</sup>는 여러가지 시도를 해보고 결과가 어떻게 나오는지 확인한다. david hume은 가장 위대한 경험주의자이며 역사상 가장 위대한 18세기 철학자다. p.114

퍼셉트론의 역사에 나타난 역설적인 사건들 중에서 가장 슬픈 일은 프랭크 로젠블랫이 1969년 체사피크 만에서 보트 사고를 당해 자기 창조물의 둘째 장(민스키가 아니라 자신이 옳았다는)을 보지 못하고 죽은 사건일 것이다. p.194

구글의 연구 부서장인 피터 노빅이 언젠가 나에게 나이브 베이즈 분류기는 구글에서 가장 널리 쓰이는 머신러닝이고 구글이 하는 일의 구석구석에 머신러닝이 적용된다고 말했다. p.252

SVM은 퍼셉트론의 일반형으로도 볼 수 있는데, 유형을 나누는 초평면의 경계는 특별한 유사성 측정(벡터 사이의 내적<sup>dot product</sup>. numpy도 `.dot` 사용)을 사용할 때 얻는 것이기 때문이다. 하지만 SVM은 다층 퍼셉트론과 비교하여 중요한 장점이 있었다. 가중치에는 국부 최적값이 많지 않고 단일한 최적값이 있으므로<sup>the weights have a single optimum instead of many local ones</sup> 신뢰성 있게 가중치를 찾기가 훨씬 더 쉽다. 이런 장점이 있으면서도 SVM의 비용은 다름 아닌 다층 퍼셉트론의 비용과 같았다. 서포트 벡터들은 실제적으로 하나의 은닉층처럼 작동하고 벡터들의 가중 평균은 출력 층처럼 작동한다. p.317

나이브 베이즈 분류기가 단 하나의 단어만 알아보는 문서에 대하여 그 단어가 학습 데이터에서는 우연히 스포츠에 관한 문서에만 있었다면, 나이브 베이즈 분류기는 그 단어를 포함한 문서를 모두 스포츠에 관한 문서로 착각하기 시작한다. 하지만 마진 최대화 방식 덕택에 SVM은 차원이 매우 높을 때조차도 과적합 문제에서 벗어날 수 있다. p.318
