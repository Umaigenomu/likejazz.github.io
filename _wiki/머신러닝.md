---
layout: wiki 
title: 머신러닝
---

<!-- TOC -->

- [모델 성능](#모델-성능)
- [데이터 과학](#데이터-과학)
- [차원 축소<sup>Dimensionality Reduction</sup>: PCA](#차원-축소dimensionality-reduction-pca)
- [Markov Chains](#markov-chains)
- [Grid Search](#grid-search)
- [Scaling, Normalization, Standardization](#scaling-normalization-standardization)
- [정보 이론](#정보-이론)
- [단순성의 원리](#단순성의-원리)
- [책](#책)
    - [Algorithms of the Intelligent Web](#algorithms-of-the-intelligent-web)
    - [마스터 알고리즘](#마스터-알고리즘)
        - [머신러닝 다섯 종족](#머신러닝-다섯-종족)
        - [발췌](#발췌)

<!-- /TOC -->

# 모델 성능

<img src="https://cdn-images-1.medium.com/max/1600/0*a2ER5Ki6Px0rFXCm.png" width="70%"> ([source](https://towardsdatascience.com/deep-misconceptions-about-deep-learning-f26c41faceec))

# 데이터 과학
<img width="50%" src="https://user-images.githubusercontent.com/1250095/33862806-e634de66-df27-11e7-8974-d5fa5cbb2652.png">

(Principles of Data Science, 2016)

책에 등장한 벤 다이어그램이 인상적이다. **Danger Zone** 표시가 재밌다. 수학 & 통계학 지식이 없다면 위험하다는 것.

# 차원 축소<sup>Dimensionality Reduction</sup>: PCA
PCA는 회전된 특징이 통계적으로 상관 관계가 없도록 데이타셋을 회전시키는 방법이다. 상관도가 높은 변수를 통합한다는 점에서 차원 축소<sup>dimensionality reduction</sup> 기법이라 한다. feature를 선별하는 것과 함께 feature engineering(extraction) 범주에 포함된다.

`PCA(whiten=True)`: This is the same as using `StandardScaler` after the transformation. whitening corresponds to not only rotating the data, but also **rescaling** it.

속성 추출 기법
- 선형 PCA<sup>principal component analysis</sup> 선형 제한
- 비선형 MDS<sup>multidimensional scaling</sup>

PCA 주로 성분 분석
1. 데이터에서 평균값 빼서 데이터를 중심에
1. 공분산<sup>covariance</sup> 매트릭스 계산
1. 공분산의 고유벡터<sup>eigenvector</sup> 계산

# Markov Chains  
- [시각화 제공](http://setosa.io/ev/markov-chains/) 이외에도 여러 시각 자료가 있는데 멋진 구현
<img src="http://docs.likejazz.com/images/2017/markov-chains.png" width="50%" />  

마코프 체인의 결과는 일정 비율로 수렴한다.

# Grid Search
하이퍼 파라미터를 결정하기 위해 다양한 파라미터로 실험을 진행해 최적의 파라미터를 찾는 과정이다. [scikit-learn의 Grid Search 문서](http://scikit-learn.org/stable/modules/grid_search.html)에 잘 정리되어 있다.

# Scaling, Normalization, Standardization
- Scaling: 서로 다른 단위의 데이터를 같은 단위로 만들어서 큰 숫자가 더 중요해보이는 왜곡을 막는 것
- Standardization(= z-score normalization): 분포를 평균 0, 표준편차 1로 바꾸는것
- Normalization(= Min-Max scaling) : 변수를 0과 1사이로 바꾸는것

스케일링을 위한 노말라이제이션을 스탠다더라이제이션으로 할 수 있어서 구분이 어렵습니다. ([출처](https://www.facebook.com/groups/TensorFlowKR/permalink/798631867144540/?comment_id=798637187144008&comment_tracking=%7B%22tn%22%3A%22R%22%7D))

# 정보 이론
정보량은 불확실성과 직접적 관련이 있다. 매우 불확실한 일이나 전혀 모르는 일을 이해하려면 많은 정보를 파악해야 한다. 반대로 이미 많이 알고 있는 일은 정보가 많지 않아도 쉽게 이해할 수 있다. 이런 점에서 정보량은 불확실성의 크기와 같다고 볼 수 있다.

> (Information) Entropy, in other words, is a measure of uncertainty. ([source](https://towardsdatascience.com/entropy-is-a-measure-of-uncertainty-e2c000301c2c))

1948년 클로드 섀넌이 그의 유명한 논문 "A Mathematics Theory of Communication"에 '정보 엔트로피' 개념을 제기하면서 정보의 단위 문제가 풀렸고, 정보의 역할을 계량화 할 수 있게 되었다. (수학의 아름다움, 2014, 2019)

월드컵 우승팀을 가리는 문제에서 브라질 처럼 우승 확률이 매우 높은 국가에 대한 사전 정보가 있다면 그 쪽을 선택하여 엔트로피를 더욱 낮출 수 있다. (수학의 아름다움에서 정리 필요)

# 단순성의 원리
* 뉴턴의 제1법칙: 관성의 법칙  
물체의 질량 중심은 외부 힘이 작용하지 않는 한 일정한 속도로 움직인다.  
우쥔은 이를 단순성의 원칙이라 표현했다.

* 오컴의 면도날(Occam's Razor 또는 Ockham's Razor)  
> simpler solutions are more likely to be correct than complex ones  
단순한 솔루션이 복잡한 솔루션 보다 정확할 가능성이 높다.

# 책
## Algorithms of the Intelligent Web
『Algorithms of the Intelligent Web』 챕터 1의 전체 노트 중 몇 가지만 따로 정리한다.
- Inference does not happen instantaneously. 추론은 순식간에 되지 않는다. 모든 데이타셋/알고리즘이 응답 시간 제한내에서 실행될 것이라고 가정하면 안된다.
- Size matters!
- Different algorithms have different scaling characteristics. Some algorithms are scalable, and **others aren’t.** 수십억 개의 뉴스 중에서 유사한 제목의 뉴스 그룹을 찾는다고 가정해보자. Not all clustering algorithms can run in parallel. 경우에 따라서는 데이타 자체를 분할해야 할 수 있다.
- Everything is not a nail! 모든 문제에 통용되는 만능 알고리즘은 없다. 이 책의 저자는 Pedro Domingos의 2012년 ACM 논문을 참고 했고, 그는 『Master Algorithms』 이라는 만능 알고리즘이 존재할 것이라는 책을 쓰긴 했지만.
- Human intuition is problematic. 직관하기 어려운 고차원에서 다변량 가우스 분포<sup>multivariate Gaussian distribution</sup>의 대부분은 평균 근처에 있지 않고 바깥<sup>shell</sup>에 있다. 낮은 차원에서 간단한 분류자를 만드는 것은 쉽지만 차원 수를 늘리면 어떤 일이 일어나는지 이해하기 어렵다.
- "garbage in, garbage out."
- Learn many different models. 넷플릭스 프라이즈의 우승자는 100개 이상의 학습기를 앙상블로 구축했다고 한다.

(Algorithms of the Intelligent Web, 2016) - *[Chapter 2](https://www.safaribooksonline.com/library/view/allgorithms-of-the/9781617292583/kindle_split_010.html)부터 다시 읽어야 한다, 2017년 12월*

## 마스터 알고리즘
### 머신러닝 다섯 종족
<img src="https://sebastianraschka.com/images/faq/classifier_categories/master_chart.jpg" width="47%" style="padding-right: 10px; float: left" /><img src="https://user-images.githubusercontent.com/1250095/57499850-13cd2200-731c-11e9-8897-b9432f0a5ede.jpeg" width="47%" />

<p style="clear:both"></p>
왼쪽 [Sebastian Raschka의 정리](https://sebastianraschka.com/faq/docs/classifier-categories.html#pedro-domingos-5-tribes-of-machine-learning)와 오른쪽은 내가 직접 책에서 촬영했다.

### 발췌
> 합리주의자 <sup>The rationalist</sup>는 첫 행동을 개시하기 전에 모든 것을 계획한다. 경험주의자 <sup>The empiricist</sup>는 여러가지 시도를 해보고 결과가 어떻게 나오는지 확인한다. david hume은 가장 위대한 경험주의자이며 역사상 가장 위대한 18세기 철학자다.  
p.114

> 퍼셉트론의 역사에 나타난 역설적인 사건들 중에서 가장 슬픈 일은 프랭크 로젠블랫이 1969년 체사피크 만에서 보트 사고를 당해 자기 창조물의 둘째 장(민스키가 아니라 자신이 옳았다는)을 보지 못하고 죽은 사건일 것이다.  
p.194

> 구글의 연구 부서장인 피터 노빅이 언젠가 나에게 나이브 베이즈 분류기는 구글에서 가장 널리 쓰이는 머신러닝이고 구글이 하는 일의 구석구석에 머신러닝이 적용된다고 말했다.  
p.252

> SVM은 퍼셉트론의 일반형으로도 볼 수 있는데, 유형을 나누는 초평면의 경계는 특별한 유사성 측정(벡터 사이의 내적<sup>dot product</sup>. numpy도 `.dot` 사용)을 사용할 때 얻는 것이기 때문이다. 하지만 SVM은 다층 퍼셉트론과 비교하여 중요한 장점이 있었다. 가중치에는 국부 최적값이 많지 않고 단일한 최적값이 있으므로<sup>the weights have a single optimum instead of many local ones</sup> 신뢰성 있게 가중치를 찾기가 훨씬 더 쉽다. 이런 장점이 있으면서도 SVM의 비용은 다름 아닌 다층 퍼셉트론의 비용과 같았다. 서포트 벡터들은 실제적으로 하나의 은닉층처럼 작동하고 벡터들의 가중 평균은 출력 층처럼 작동한다.  
p.317

> 나이브 베이즈 분류기가 단 하나의 단어만 알아보는 문서에 대하여 그 단어가 학습 데이터에서는 우연히 스포츠에 관한 문서에만 있었다면, 나이브 베이즈 분류기는 그 단어를 포함한 문서를 모두 스포츠에 관한 문서로 착각하기 시작한다. 하지만 마진 최대화 방식 덕택에 SVM은 차원이 매우 높을 때조차도 과적합 문제에서 벗어날 수 있다.  
p.318
