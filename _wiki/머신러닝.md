---
layout: wiki 
title: 머신러닝
last-modified: 2020/04/16 19:49:23
---

<!-- TOC -->

- [개요](#개요)
- [데이터 과학](#데이터-과학)
- [Improving ML inferences](#improving-ml-inferences)
- [차원 축소<sup>Dimensionality Reduction</sup>: PCA](#차원-축소dimensionality-reduction-pca)
- [Markov Chains](#markov-chains)
- [Grid Search](#grid-search)
- [Scaling, Normalization, Standardization](#scaling-normalization-standardization)
- [정보 이론](#정보-이론)
- [단순성의 원리](#단순성의-원리)
- [Books](#books)
    - [Algorithms of the Intelligent Web <sub>2009</sub>](#algorithms-of-the-intelligent-web-2009)
    - [마스터 알고리즘 <sub>2015, 2016</sub>](#마스터-알고리즘-2015-2016)
    - [기계는 어떻게 생각하는가? <sub>2018, 2019</sub>](#기계는-어떻게-생각하는가-2018-2019)
        - [넷플릭스 프라이즈와 인공 지능](#넷플릭스-프라이즈와-인공-지능)
        - [행렬 인수 분해 <sup>matirx factorization</sup>](#행렬-인수-분해-matirx-factorization)
        - [시간에 따른 평점 예측](#시간에-따른-평점-예측)

<!-- /TOC -->

# 개요
<img src="https://developer.nvidia.com/sites/default/files/Deep_Learning_Icons_R5_PNG.jpg.png" width="80%"> 출처: NVIDIA

<img src="https://cdn-images-1.medium.com/max/1600/0*a2ER5Ki6Px0rFXCm.png" width="80%">[^fn-data]

[^fn-data]: <https://towardsdatascience.com/deep-misconceptions-about-deep-learning-f26c41faceec>

# 데이터 과학
<img width="40%" src="https://user-images.githubusercontent.com/1250095/33862806-e634de66-df27-11e7-8974-d5fa5cbb2652.png">

(Principles of Data Science, 2016)

책에 등장한 벤 다이어그램이 인상적이다. **Danger Zone** 표시가 재밌다. 수학, 통계학 지식이 없다면 위험하다는 것.

# Improving ML inferences
<img width="70%" src="https://user-images.githubusercontent.com/1250095/72658609-ae771800-39f6-11ea-8b46-5592a668a086.png">

[링크](https://medium.com/apache-mxnet/faster-cheaper-leaner-improving-real-time-ml-inference-using-apache-mxnet-2ee245668b55)

# 차원 축소<sup>Dimensionality Reduction</sup>: PCA
PCA는 회전된 특징이 통계적으로 상관 관계가 없도록 데이타셋을 회전시키는 방법이다. 상관도가 높은 변수를 통합한다는 점에서 차원 축소<sup>dimensionality reduction</sup> 기법이라 한다. feature를 선별하는 것과 함께 feature engineering(extraction) 범주에 포함된다.

`PCA(whiten=True)`: This is the same as using `StandardScaler` after the transformation. whitening corresponds to not only rotating the data, but also **rescaling** it.

속성 추출 기법
- 선형 PCA<sup>principal component analysis</sup> 선형 제한
- 비선형 MDS<sup>multidimensional scaling</sup>

PCA 주로 성분 분석
1. 데이터에서 평균값 빼서 데이터를 중심에
1. 공분산<sup>covariance</sup> 매트릭스 계산
1. 공분산의 고유벡터<sup>eigenvector</sup> 계산

# Markov Chains  
- [시각화 제공](http://setosa.io/ev/markov-chains/) 이외에도 여러 시각 자료가 있는데 멋진 구현
<img src="http://docs.likejazz.com/images/2017/markov-chains.png" width="50%" />  

마코프 체인의 결과는 일정 비율로 수렴한다.

# Grid Search
하이퍼 파라미터를 결정하기 위해 다양한 파라미터로 실험을 진행해 최적의 파라미터를 찾는 과정이다. [scikit-learn의 Grid Search 문서](http://scikit-learn.org/stable/modules/grid_search.html)에 잘 정리되어 있다.

# Scaling, Normalization, Standardization
- Scaling: 서로 다른 단위의 데이터를 같은 단위로 만들어서 큰 숫자가 더 중요해보이는 왜곡을 막는 것
- Standardization(= z-score normalization): 분포를 평균 0, 표준편차 1로 바꾸는것
- Normalization(= Min-Max scaling) : 변수를 0과 1사이로 바꾸는것

스케일링을 위한 노말라이제이션을 스탠다더라이제이션으로 할 수 있어서 구분이 어렵습니다. ([출처](https://www.facebook.com/groups/TensorFlowKR/permalink/798631867144540/?comment_id=798637187144008&comment_tracking=%7B%22tn%22%3A%22R%22%7D))

# 정보 이론
정보량은 불확실성과 직접적 관련이 있다. 매우 불확실한 일이나 전혀 모르는 일을 이해하려면 많은 정보를 파악해야 한다. 반대로 이미 많이 알고 있는 일은 정보가 많지 않아도 쉽게 이해할 수 있다. 이런 점에서 정보량은 불확실성의 크기와 같다고 볼 수 있다.

> (Information) Entropy, in other words, is a measure of uncertainty. ([source](https://towardsdatascience.com/entropy-is-a-measure-of-uncertainty-e2c000301c2c))

1948년 클로드 섀넌이 그의 유명한 논문 "A Mathematics Theory of Communication"에 '정보 엔트로피' 개념을 제기하면서 정보의 단위 문제가 풀렸고, 정보의 역할을 계량화 할 수 있게 되었다. (수학의 아름다움, 2014, 2019)

월드컵 우승팀을 가리는 문제에서 브라질 처럼 우승 확률이 매우 높은 국가에 대한 사전 정보가 있다면 그 쪽을 선택하여 엔트로피를 더욱 낮출 수 있다. (수학의 아름다움에서 정리 필요)

# 단순성의 원리
* 뉴턴의 제1법칙: 관성의 법칙  
물체의 질량 중심은 외부 힘이 작용하지 않는 한 일정한 속도로 움직인다.  
우쥔은 이를 단순성의 원칙이라 표현했다.

* 오컴의 면도날(Occam's Razor 또는 Ockham's Razor)  
> simpler solutions are more likely to be correct than complex ones  
단순한 솔루션이 복잡한 솔루션 보다 정확할 가능성이 높다.

# Books
## Algorithms of the Intelligent Web <sub>2009</sub>
『Algorithms of the Intelligent Web』 챕터 1의 전체 노트 중 몇 가지만 따로 정리한다.
- Inference does not happen instantaneously. 추론은 순식간에 되지 않는다. 모든 데이타셋/알고리즘이 응답 시간 제한내에서 실행될 것이라고 가정하면 안된다.
- Size matters!
- Different algorithms have different scaling characteristics. Some algorithms are scalable, and **others aren’t.** 수십억 개의 뉴스 중에서 유사한 제목의 뉴스 그룹을 찾는다고 가정해보자. Not all clustering algorithms can run in parallel. 경우에 따라서는 데이타 자체를 분할해야 할 수 있다.
- Everything is not a nail! 모든 문제에 통용되는 만능 알고리즘은 없다. 이 책의 저자는 Pedro Domingos의 2012년 ACM 논문을 참고 했고, 그는 『Master Algorithms』 이라는 만능 알고리즘이 존재할 것이라는 책을 쓰긴 했지만.
- Human intuition is problematic. 직관하기 어려운 고차원에서 다변량 가우스 분포<sup>multivariate Gaussian distribution</sup>의 대부분은 평균 근처에 있지 않고 바깥<sup>shell</sup>에 있다. 낮은 차원에서 간단한 분류자를 만드는 것은 쉽지만 차원 수를 늘리면 어떤 일이 일어나는지 이해하기 어렵다.
- "garbage in, garbage out."
- Learn many different models. 넷플릭스 프라이즈의 우승자는 100개 이상의 학습기를 앙상블로 구축했다고 한다.

(Algorithms of the Intelligent Web, 2016) - *[Chapter 2](https://www.safaribooksonline.com/library/view/allgorithms-of-the/9781617292583/kindle_split_010.html)부터 다시 읽어야 한다, 2017년 12월*

## 마스터 알고리즘 <sub>2015, 2016</sub>
머신러닝 다섯 종족  
<img src="https://sebastianraschka.com/images/faq/classifier_categories/master_chart.jpg" width="47%" style="padding-right: 10px; float: left" /><img src="https://user-images.githubusercontent.com/1250095/57499850-13cd2200-731c-11e9-8897-b9432f0a5ede.jpeg" width="47%" />

<p style="clear:both"></p>
왼쪽 [Sebastian Raschka의 정리](https://sebastianraschka.com/faq/docs/classifier-categories.html#pedro-domingos-5-tribes-of-machine-learning)와 오른쪽은 내가 직접 책에서 촬영했다.

합리주의자 <sup>The rationalist</sup>는 첫 행동을 개시하기 전에 모든 것을 계획한다. 경험주의자 <sup>The empiricist</sup>는 여러가지 시도를 해보고 결과가 어떻게 나오는지 확인한다. david hume은 가장 위대한 경험주의자이며 역사상 가장 위대한 18세기 철학자다. p.114

퍼셉트론의 역사에 나타난 역설적인 사건들 중에서 가장 슬픈 일은 프랭크 로젠블랫이 1969년 체사피크 만에서 보트 사고를 당해 자기 창조물의 둘째 장(민스키가 아니라 자신이 옳았다는)을 보지 못하고 죽은 사건일 것이다. p.194

구글의 연구 부서장인 피터 노빅이 언젠가 나에게 나이브 베이즈 분류기는 구글에서 가장 널리 쓰이는 머신러닝이고 구글이 하는 일의 구석구석에 머신러닝이 적용된다고 말했다. p.252

SVM은 퍼셉트론의 일반형으로도 볼 수 있는데, 유형을 나누는 초평면의 경계는 특별한 유사성 측정(벡터 사이의 내적<sup>dot product</sup>. numpy도 `.dot` 사용)을 사용할 때 얻는 것이기 때문이다. 하지만 SVM은 다층 퍼셉트론과 비교하여 중요한 장점이 있었다. 가중치에는 국부 최적값이 많지 않고 단일한 최적값이 있으므로<sup>the weights have a single optimum instead of many local ones</sup> 신뢰성 있게 가중치를 찾기가 훨씬 더 쉽다. 이런 장점이 있으면서도 SVM의 비용은 다름 아닌 다층 퍼셉트론의 비용과 같았다. 서포트 벡터들은 실제적으로 하나의 은닉층처럼 작동하고 벡터들의 가중 평균은 출력 층처럼 작동한다. p.317

나이브 베이즈 분류기가 단 하나의 단어만 알아보는 문서에 대하여 그 단어가 학습 데이터에서는 우연히 스포츠에 관한 문서에만 있었다면, 나이브 베이즈 분류기는 그 단어를 포함한 문서를 모두 스포츠에 관한 문서로 착각하기 시작한다. 하지만 마진 최대화 방식 덕택에 SVM은 차원이 매우 높을 때조차도 과적합 문제에서 벗어날 수 있다. p.318

## 기계는 어떻게 생각하는가? <sub>2018, 2019</sub>
### 넷플릭스 프라이즈와 인공 지능  
<img src="https://user-images.githubusercontent.com/1250095/79428346-fced8000-8000-11ea-97cc-1af3c4377453.png" width="60%">

벨코의 기본 모델은 사용자 전체를 대상으로 놓고 보면 그럴 듯 했지만 훌륭하다고는 할 수 없었다. 이는 단일 모델이 모든 사람에게 적용될 수 없다는 사실을 보여준다. 미국 공군은 1950년대에 발생한 수많은 항공기 사고를 조사하면서 이와 동일한 현상을 발견했다. 1920년대부터 항공기 조종석은 미국인 남성의 평균 체형에 맞추어 제작했다. 이 문제를 연구했던 길버트 대니얼스<sup>Gilbert Daniels</sup> 중위는 미국인 남성 대부분의 체형이 평균에 가깝지 않다는 것을 발견했다. 토드 로즈 <sup>Todd Rose</sup>는 『평균의 종말』이라는 책에서 이러한 상황을 다음과 같이 설명했다.

> 조종사 4,063명의 신체 치수 10개 항목을 측정해 평균을 구했으나 조종사 중 모든 신체 치수가 이 10개 항목의 평균 범위에 들어가는 조종사는 한 명도 없었다. 어떤 조종사의 팔은 평균 보다 길지만 다리는 평균보다 짧았다. 어떤 조종사는 가슴둘레가 컸지만 엉덩이가 작았다. 놀랍게도 대니얼스는 10개의 치수 중 세 개(목둘레, 허벅지 둘레, 손목 둘레)만 선택한 경우조차 조종사의 3.5% 이하만 평균 범위에 들어간다는 사실을 발견했다. 대니얼스의 발견이 시사하는 바는 명확했다. 평균 체형의 조종사는 존재하지 않는다. 조종사의 평균 체형에 맞추어 설계한 조종석은 어떤 조종사의 체형에도 맞지 않는다.

이 발견을 토대로 대니얼스는 조종사가 자신의 체형에 맞게 조절할 수 있는 조종석을 제안했으며 공군을 이를 수용했다. 공군은 평균을 표준으로 삼는 방식을 포기함으로써 개인형 맞춤이라는 새로운 원칙을 제시했다. p.119

'터미네이터 효과'를 잡아낼 수 있어야 했다. 터미네이터 효과란, 넷플릭스 사용자 중에는 SF와 액션 영화를 좋아하는 그룹이 있고, 어린이 영화를 좋아하는 그룹이 있고, 이 두 그룹 모두에 포함되는 사용자도 있고, 이 두 그룹에 포함되지 않는 사용자도 있는 상황을 설명하는 것이다. 터미네이터 효과를 해결하기 위해 참가자들은 대부분 행렬 인수 분해라는 방식을 사용했다. p.120

### 행렬 인수 분해 <sup>matirx factorization</sup>  
<img src="https://user-images.githubusercontent.com/1250095/79434795-d7b13f80-8009-11ea-9f17-72c110ec6df7.png" width="50%">

행렬 인수 분해는 거대한 행렬이 중복된 정보를 많이 갖고 있다는 사실을 기반으로 한다.

'쥬라기 공원'이 각 장르에 해당하는지를 나타내는 숫자에 스티븐 스필버그가 각 장르를 얼마나 좋아하는지 나타내는 숫자를 곱한 뒤, 이 숫자들을 모두 합산하여 스필버그가 '쥬라기 공원'을 얼마나 좋아하는지를 나타내는 점수를 구한다. p.122

<img src="https://user-images.githubusercontent.com/1250095/79435354-7ccc1800-800a-11ea-9843-d72740c41e3c.png" width="50%">

지금 살펴본 방법이 행렬 인수 분해다. 이 알고리즘은 개인화된 추천 기능을 만들기 위해 살펴볼 만한 가장 중요한 알고리즘이며 꼭 습득해야 하는 핵심 개념이다. 이 알고리즘에 행렬 인수 분해라는 이름이 붙은 이유는, 영화와 사용자에 대한 정보를 나타내는 숫자가 담긴 두 개 이상의 행렬(인수)을 곱해 위위 이미지 처럼 거대한 평점 행렬과 같은 원시 데이터를 만들어 그 데이터로부터 근사치를 계산하는 방식이기 때문이다. p.123

대회가 열렸던 첫해부터 널리 쓰인 모델은 ML@UToronto 팀의 연구원들이 개발한 인공 신경망이었다. 이 신경망은 수학적으로 행렬 인수 분해와 매우 유사했지만 존재하지 않는 평점 처리를 다른 방식으로 했으며, 평점을 1.0과 5.0 사이의 실수 대신에 1, 2, 3, 4, 5라는 다섯 개의 불연속적인 범주로 취급했다. p.134

### 시간에 따른 평점 예측  
넷플릭스 프라이즈가 열린 두 번째 해에 참여한 팀들은 사용자가 '언제' 영화에 평점을 매겼는지도 주목하기 시작했다. p.135

<img src="https://user-images.githubusercontent.com/1250095/79438577-ad15b580-800e-11ea-9002-dd4188afa538.png" width="50%">

넷플릭스 프라이즈의 데이터는 데이터 마이닝의 보물 창고 였다. 한 가지 사례를 더 들면, 빅카오스 팀은 영화 제목에 숫자가 포함되어 있는지 여부로 사용자의 호감을 예측할 수 있다는 것을 발견했다(효과는 미미했지만 이러한 현상이 분명히 존재했다). p.138