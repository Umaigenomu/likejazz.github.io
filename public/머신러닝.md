# 알고리즘
## 분류
|  | 모델 | 태스크 |
|-------------|------------------|-----------|
| 지도 학습 | 최근접 이웃<sup>k-Nearest Neighbors</sup> | 분류 |
|  | 나이브 베이즈<sup>Naive Bayes</sup> | 분류 |
|  | 결정 트리<sup>Decision Trees</sup> | 분류 |
|  | 선형회귀<sup>Linear Regression</sup> | 수치 예측 |
|  | 회귀 트리<sup>Regression Tree</sup>[^18] | 수치 예측 |
|  | 모델 트리<sup>Model Tree</sup> | 수치 예측 |
|  | 신경망<sup>Neural Network</sup> | 다중 용도 |
|  | 서포트 벡터 머신<sup>Support Vector Machine</sup> | 다중 용도 |
| 비지도 학습 | 연관 규칙<sup>Association Rule</sup>[^17] | 패턴 탐지 |
|  | K평균 군집화<sup>k-Means Clustering</sup> | 군집화 |

[^4]

[^4]: http://1004jonghee.tistory.com/entry/R-%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5
[^17]: https://github.com/scikit-learn/scikit-learn/issues/2662
[^18]: http://stackoverflow.com/questions/11810949/difference-between-regression-tree-and-model-tree

## Decision Trees
- alternative to kernels
- easy for human interpretation
- useful for ensemble learning - random forest

노드에 포함된 모든 exmaple들이 원하는 y값에 대해 같은 y값을 가지고 있을때 pure하다.[^7]

[^7]: http://www.kmooc.kr/courses/course-v1:KAISTk+KCS470+2016_K0201/about

scikit-learn으로 구현하면 decision rules를 직접 볼 수가 없는데(따라서 룰을 쉽게 수정할 수 있는 random forest의 장점도 많이 희석) SO에 extract 방법이 있음[^10] 아래는 IRIS 데이타셋으로 decision trees를 100개 만들었고[^12] 그 중 하나를 dot export[^14] 하여 graphviz viewer[^13]로 확인한 모습.

[^10]: http://stackoverflow.com/a/39772170/3513266
[^11]: http://planspace.org/20151129-see_sklearn_trees_with_d3/
[^12]: http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html#sphx-glr-auto-examples-ensemble-plot-forest-iris-py
[^13]: http://www.webgraphviz.com/
[^14]: http://stackoverflow.com/questions/27817994/visualizing-decision-tree-in-scikit-learn

<img src="http://docs.likejazz.com/images/2017/graphviz.png" width="50%" />

`samples`는 해당 decision의 샘플 수, `[Setosa, Versicolor, Virginica]` 결과에 해당하는 값이 `values = [63,41,46]` decision trees는 value의 합이 samples의 수와 일치하지만 이 경우는 random forest라 일치하지 않음. `class_names`를 지정하면 아래처럼 결과를 볼 수 있음. 아래는 titanic kaggle 데이타를 decision trees로 표현. random forest가 아니기 때문에 values의 합이 samples의 수와 일치.

<img src="http://docs.likejazz.com/images/2017/decisiontrees.png" width="50%" />

분류 기준은 디폴트로 gini impurity를 사용하며 entropy를 사용하도록 변경 가능  
- `$$\textit{Gini}: \mathit{Gini}(E) = 1 - \sum_{j=1}^{c}p_j^2$$`
- `$$\textit{Entropy}: H(E) = -\sum_{j=1}^{c}p_j\log p_j$$`

<img src="https://abhyast.files.wordpress.com/2015/01/image9.png" width="50%" />

scikit-learn 디폴트인 CART 알고리즘은 gini 사용. 

decision tree를 d3로 구현하는 방법[^11]도 있으며, `tree.tree_.threshold[x]`의 값을 직접 수정하여 decision rule을 변경하는 것도 가능. 아래는 scikit-learn의 random forest 예제[^12]를 원본 decision tree 적용(상단), decision tree의 rule을 수정(하단)하여 decision boundaries의 변경을 도식화 한 모습

<img src="http://docs.likejazz.com/images/2017/iris-rf-decision-boundaries.png" width="70%" />

### ID3 알고리즘<sub>Ross Quinlan, 1986</sub>[^8]  

[^8]: https://en.wikipedia.org/wiki/ID3_algorithm

choosing best attr
- entropy<sup>복잡도</sup> = `$$H(S)=\sum_{x\in X}{p(x)\log_{2}{(1/p(x))}}$$`
  - p(x): 집합 S의 원소 수에 대한 클래스 x의 원소 수 비율. 상기 수식을 아래 그래프는 `$$p(x)\log_{2}{(1/p(x))}$$` 값만 따로 표현한 경우. 비율 상위 35% 지점에서 최대가 된다. 0%/100%에서는 0. gini impurity의 경우는 `$$p(x)(1-p(x))$$`이므로 50%에서 최대.

<img src="http://docs.likejazz.com/images/2017/entropy.png" width="50%" />

  > When H(S)=0, the set S is perfectly classified (i.e. all elements in S are of the same class).

  위 인용문은 pure한 경우를 말하며 결정 요인이 됨.

- information gain<sup>정보량, 평균 정보량의 기대치</sup>  
`$$IG(A,S)=H(S)-\sum_{ t\in T }{ p(t)H(t) }$$`  
속성 A로 집합 S를 분리했을때 S의 불확실성이 얼마나 감소했는지. 이 값이 커야 결정 노드로 선정.
  - H(S): 집합 S의 엔트로피
  - H(t): 부분집합 t의 엔트로피

일본어 위키에 유도 과정이 잘 나와 있음[^9] IG 최대값 부터 노드로 선정하고 entropy가 0이 되는 x를 pure한 결정 요인으로 선정. 계산은 wolfram alpha 이용. 그러나 decision trees의 문제는 오버피팅이 너무 심하다.

[^9]: https://ja.wikipedia.org/wiki/ID3

scikit-learn uses an optimised version of the CART(is very similar to C4.5) algorithm.

## Random Forest<sub>Leo Breiman et al. 2001</sub>
- bagging trees
  - b=1...B random sampling
  - tree 구성(ID3)
  - classification B개 모든 tree를 사용해서 분류, majority vote로 결정.
  - decrease variance while bias stays same.
- attributes random

특징
- easy to interpret. 딥러닝 과는 정반대
- can induce non-linear decision boundaries.
- fast at prediction(O(height of tree))

## Naive<sup>순박한</sup> Bayes
- Naive Bayes: based on applying Bayes' theorem with strong (naive) independence assumptions between the features.
- Bayes' theorem(Bayes' rule)  
<img src="http://strangenotions.com/wp-content/uploads/BayesTheorem-600x319.jpg" width="50%" />

Multinomial Naive Bayes 전개 및 scikit-learn과 일치하는 결과 비교  
[multinomial-naive-bayes.ipynb](https://nbviewer.jupyter.org/github/likejazz/likejazz.github.io/blob/master/public/notebooks/multinomial-naive-bayes.ipynb)

이후 [다항 분포 나이브 베이즈 알고리즘 전개](http://docs.likejazz.com/multinomial-naive-bayes/) 문서로 정리

조건부 확률 수식 `$$ P(S \mid W) $$` W가 주어졌을때 S가 발생할 확률  
- `$$ P(S\mid W)=\frac { P(S\bigcap { W } ) }{ P(W) } $$`
- `$$ P(S\bigcap { W } ) = P(S\mid W) { P(W) } $$`

> 두 사건이 서로 독립적이라면 두 사건이 같이 일어날 곱사건은 아래와 같이 각 사건이 일어날 확률을 각각 구한 다음 곱한다. 하지만 두 사건이 독립적이지 않다면, 그냥 두 사건이 일어날 확률을 개별적으로 구해서 곱할 수 없다. <확률적 프로그래밍 기초 원리> p.61

- `$$ P(O\bigcap { H } ) = P(O\mid H) P(H) = P(O)P(H) $$`

<img src="http://docs.likejazz.com/images/2017/bayesian-prob.jpg" width="70%" />

상기 네온사인 이미지의 수식을 보면,
- P(A|B) 사후 확률
- P(B|A) 가능도
- P(B) 증거
- P(A) 사전 확률

사후 확률 오즈 posterior odds R = P(수두|붉은점) / P(천연두|붉은점) = 0.988/0.011 = 90  
일반적으로 3보다 크거나 1/3보다 작을 때 두 가설이 의미 있는 차이가 있다고 본다.

## k-Nearest Neighbors(KNN)
k개의 최근접 이웃 사이에서 가장 많은 항목으로 분류. '가까움'은 유클리드 거리<sup>Euclidean distance</sup> 주로 사용.

복잡한 벡터에서 k가 클수록 오버피팅이 줄어들어 부드러운 경계가 생긴다.  

## Markov Chains  
- [시각화 제공](http://setosa.io/ev/markov-chains/) 이외에도 여러 시각 자료가 있는데 멋진 구현
<img src="http://docs.likejazz.com/images/2017/markov-chains.png" width="50%" />  

마코프 체인의 결과는 일정 비율로 수렴한다.

### Hidden Markov Models
정리 필요

## Support Vector Machine
linear classifier  
- margin: maximize, shattering dataset 낮춘다. -> vc dimension 낮춘다. -> true error 낮춘다. 정확도가 높아진다.
- support vector
- kernels: non-linear를 linear하게 할 수 있게 한다.

<img src="http://docs.likejazz.com/images/2017/svm1.png" width="47%" style="padding-right: 10px; float: left" /><img src="http://docs.likejazz.com/images/2017/svm2.png" width="47%" />

가우시안 커널<sup>(Gaussian)Radial Basis Function Kernel</sup>을 적용한 SVM 분류 도식화[^5]

[^5]: https://gist.github.com/mblondel/586753

가우시안 커널 TeX: `$$e^{ {-\left({x-\mu}\right)^{2} }/{2\sigma^{2} } }$$`  
원래 SVM은 공간 왜곡을 통해 linear한 hyperplane으로 데이타 구분. 
<img src="http://cfile30.uf.tistory.com/image/2646603B56E062DF07BEFC" width="70%" />[^6]  

[^6]: http://gentlej90.tistory.com/44

NN은 hidden layer로 문제를 해결하고 SVM은 차원을 늘리는 방법으로 해결하지만 차원수를 높였기 때문에 발생하는 계산량 증가와 같은 부작용도 고려해야 하는데 이러한 부작용을 해결하기 위한 방법이 커널 트릭이다.[^6] 커널 트릭을 통해 아래와 같이 p차원 설명벡터 x를 힐버트 공간의 `$$\Phi(x)$$`로 옮긴다.  
<img src="https://upload.wikimedia.org/wikipedia/commons/1/1b/Kernel_Machine.png" width="70%" />

SVM을 만든 Vladimir N. Vapnik은 80세가 넘는 나이에 2014년 페이스북 AI 리서치 합류

# 기타

Python Libraries for Computer Vision
- OpenCV
- mahotas
- skimage  

OpenCV의 성능이 가장 좋다고[^3]

[^3]: http://kampta.github.io/Performance-Shootout-mahotas-vs-skimage-vs-opencv-part1/

Smoothing   
- `+1`, additive smoothing, Laplace<sup>라플라스</sup> smoothing

# 활용 예제
## 뉴스 카테고리 분류
Kaggle의 뉴스 데이타 이용, 뉴스 제목을 이용해 카테고리를 분류하는 실험 진행. one-hot vector라 decision tree 룰을 수정하는게 의미 없고, random forest 또한 화이트박스 이지만 룰을 수정하기가 쉽지 않다. multinomial naive bayes에서 90%가 넘는 가장 높은 정확도 기록. 실행 속도도 NB가 가장 빠르다.

[news-classification.ipynb](https://nbviewer.jupyter.org/github/likejazz/likejazz.github.io/blob/master/public/notebooks/news-classification.ipynb)

깃헙 노트북에는 lime 그래프가 보이지 않음. nbviewer를 통하면 잘 보인다.

naive bayes간 비교도 진행해보았는데 마찬가지로 multinomial nb가 가장 빠르며 가장 정확. bernoulli nb가 아주 약간 더 높으나 변별력 없는 수치. gaussian nb의 경우 수식이 복잡해 가장 느림에도 불구하고 가장 낮은 정확도 기록.

[news-classification-naive-bayes.ipynb](https://nbviewer.jupyter.org/github/likejazz/likejazz.github.io/blob/master/public/notebooks/news-classification-nb.ipynb)

Multinomial NB<sup>다항분포 나이브 베이즈</sup>는 매닝의 IIR책 챕터 13에 상세히 소개되어 있음. [다항 분포 나이브 베이즈 알고리즘 전개](http://docs.likejazz.com/multinomial-naive-bayes/) 정리
