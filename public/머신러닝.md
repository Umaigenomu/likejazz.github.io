# 알고리즘
## 분류
|  | 모델 | 태스크 |
|-------------|------------------|-----------|
| 지도 학습 | 최근접 이웃<sup>k-Nearest Neighbors</sup> | 분류 |
|  | 나이브 베이즈<sup>Naive Bayes</sup> | 분류 |
|  | 결정 트리<sup>Decision Trees</sup> | 분류 |
|  | 선형회귀<sup>Linear Regression</sup> | 수치 예측 |
|  | 회귀 트리 | 수치 예측 |
|  | 모델 트리 | 수치 예측 |
|  | 신경망<sup>Neural Network</sup> | 다중 용도 |
|  | 서포트 벡터 머신<sup>Support Vector Machine</sup> | 다중 용도 |
| 비지도 학습 | 연관 규칙 | 패턴 탐지 |
|  | K평균 군집화<sup>k-Means Clustering</sup> | 군집화 |

[^4]

[^4]: <http://1004jonghee.tistory.com/entry/R-%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5>

## Decision Trees
- alternative to kernels
- easy for human interpretation
- useful for ensemble learning - random forest

노드에 포함된 모든 exmaple들이 원하는 y값에 대해 같은 y값을 가지고 있을때 pure하다.[^7]

[^7]: http://www.kmooc.kr/courses/course-v1:KAISTk+KCS470+2016_K0201/about

scikit-learn으로 구현하면 decision rules를 직접 볼 수가 없는데(따라서 룰을 쉽게 수정할 수 있는 random forest의 장점도 많이 희석) SO에 extract 방법이 있음[^10] 아래는 IRIS 데이타셋으로 decision trees를 100개 만들었고[^12] 그 중 하나를 dot export[^14] 하여 graphviz viewer[^13]로 확인한 모습.

[^10]: http://stackoverflow.com/a/39772170/3513266
[^11]: http://planspace.org/20151129-see_sklearn_trees_with_d3/
[^12]: http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html#sphx-glr-auto-examples-ensemble-plot-forest-iris-py
[^13]: http://www.webgraphviz.com/
[^14]: http://stackoverflow.com/questions/27817994/visualizing-decision-tree-in-scikit-learn

<img src="http://docs.likejazz.com/images/2017/graphviz.png" width="50%" />

`samples`는 해당 decision의 샘플 수, `[Setosa, Versicolor, Virginica]` 결과에 해당하는 값이 `values = [63,41,46]` decision trees는 value의 합이 samples의 수와 일치하지만 이 경우는 random forest라 일치하지 않음. `class_names`를 지정하면 아래처럼 결과를 볼 수 있음. 아래는 titanic kaggle 데이타를 decision trees로 표현. random forest가 아니기 때문에 values의 합이 samples의 수와 일치.

<img src="http://docs.likejazz.com/images/2017/decisiontrees.png" width="50%" />

분류 기준은 디폴트로 gini impurity를 사용하며 entropy를 사용하도록 변경 가능  
- `$$\textit{Gini}: \mathit{Gini}(E) = 1 - \sum_{j=1}^{c}p_j^2$$`
- `$$\textit{Entropy}: H(E) = -\sum_{j=1}^{c}p_j\log p_j$$`

<img src="https://abhyast.files.wordpress.com/2015/01/image9.png" width="50%" />

scikit-learn 디폴트인 CART 알고리즘은 gini 사용. 

decision tree를 d3로 구현하는 방법[^11]도 있으며, `tree.tree_.threshold[x]`의 값을 직접 수정하여 decision rule을 변경하는 것도 가능. 아래는 scikit-learn의 random forest 예제[^12]를 원본 decision tree 적용(상단), decision tree의 rule을 수정(하단)하여 decision boundaries의 변경을 도식화 한 모습

<img src="http://docs.likejazz.com/images/2017/iris-rf-decision-boundaries.png" width="70%" />

### ID3 알고리즘<sub>Ross Quinlan, 1986</sub>[^8]  

[^8]: https://en.wikipedia.org/wiki/ID3_algorithm

choosing best attr
- entropy<sup>복잡도</sup> = `$$H(S)=\sum_{x\in X}{p(x)\log_{2}{(1/p(x))}}$$`
  - p(x): 집합 S의 원소 수에 대한 클래스 x의 원소 수 비율. 상기 수식을 아래 그래프는 `$$p(x)\log_{2}{(1/p(x))}$$` 값만 따로 표현한 경우. 비율 상위 35% 지점에서 최대가 된다. 0%/100%에서는 0. gini impurity의 경우는 `$$p(x)(1-p(x))$$`이므로 50%에서 최대.

<img src="http://docs.likejazz.com/images/2017/entropy.png" width="50%" />

  > When H(S)=0, the set S is perfectly classified (i.e. all elements in S are of the same class).

  위 인용문은 pure한 경우를 말하며 결정 요인이 됨.

- information gain<sup>정보량, 평균 정보량의 기대치</sup>  
`$$IG(A,S)=H(S)-\sum_{ t\in T }{ p(t)H(t) }$$`  
속성 A로 집합 S를 분리했을때 S의 불확실성이 얼마나 감소했는지. 이 값이 커야 결정 노드로 선정.
  - H(S): 집합 S의 엔트로피
  - H(t): 부분집합 t의 엔트로피

일본어 위키에 유도 과정이 잘 나와 있음[^9] IG 최대값 부터 노드로 선정하고 entropy가 0이 되는 x를 pure한 결정 요인으로 선정. 계산은 wolfram alpha 이용. 그러나 decision trees의 문제는 오버피팅이 너무 심하다.

[^9]: https://ja.wikipedia.org/wiki/ID3

scikit-learn uses an optimised version of the CART(is very similar to C4.5) algorithm.

## Random Forest<sub>Leo Breiman et al. 2001</sub>
- bagging trees
  - b=1...B random sampling
  - tree 구성(ID3)
  - classification B개 모든 tree를 사용해서 분류, majority vote로 결정.
  - decrease variance while bias stays same.
- attributes random

특징
- easy to interpret. 딥러닝 과는 정반대
- can induce non-linear decision boundaries.
- fast at prediction(O(height of tree))

## Naive<sup>순박한</sup> Bayes
정리 필요

## k-Nearest Neighbors(KNN)
k개의 최근접 이웃 사이에서 가장 많은 항목으로 분류. '가까움'은 유클리드 거리<sup>Euclidean distance</sup> 주로 사용.

복잡한 벡터에서 k가 클수록 오버피팅이 줄어들어 부드러운 경계가 생긴다.  

## Markov Chains  
- [시각화 제공](http://setosa.io/ev/markov-chains/) 이외에도 여러 시각 자료가 있는데 멋진 구현
<img src="http://docs.likejazz.com/images/2017/markov-chains.png" width="50%" />  

마코프 체인의 결과는 일정 비율로 수렴한다.

### Hidden Markov Models
정리 필요

## Support Vector Machine
linear classifier  
- margin: maximize, shattering dataset 낮춘다. -> vc dimension 낮춘다. -> true error 낮춘다. 정확도가 높아진다.
- support vector
- kernels: non-linear를 linear하게 할 수 있게 한다.

<img src="http://docs.likejazz.com/images/2017/svm1.png" width="47%" style="padding-right: 10px; float: left" /><img src="http://docs.likejazz.com/images/2017/svm2.png" width="47%" />

가우시안 커널<sup>(Gaussian)Radial Basis Function Kernel</sup>을 적용한 SVM 분류 도식화[^5]

[^5]: https://gist.github.com/mblondel/586753

가우시안 커널 TeX: `$$e^{{{-\left({x-\mu}\right)^{2}}/{2\sigma^{2}}}}$$`  
원래 SVM은 공간 왜곡을 통해 linear한 hyperplane으로 데이타 구분. 
<img src="http://cfile30.uf.tistory.com/image/2646603B56E062DF07BEFC" width="70%" />[^6]  

[^6]: http://gentlej90.tistory.com/44

NN은 hidden layer로 문제를 해결하고 SVM은 차원을 늘리는 방법으로 해결하지만 차원수를 높였기 때문에 발생하는 계산량 증가와 같은 부작용도 고려해야 하는데 이러한 부작용을 해결하기 위한 방법이 커널 트릭이다.[^6] 커널 트릭을 통해 아래와 같이 p차원 설명벡터 x를 힐버트 공간의 `$$\Phi(x)$$`로 옮긴다.  
<img src="https://upload.wikimedia.org/wikipedia/commons/1/1b/Kernel_Machine.png" width="70%" />

SVM을 만든 Vladimir N. Vapnik은 80세가 넘는 나이에 2014년 페이스북 AI 리서치 합류

# 기타

Python Libraries for Computer Vision
- OpenCV
- mahotas
- skimage  

OpenCV의 성능이 가장 좋다고[^3]

[^3]: http://kampta.github.io/Performance-Shootout-mahotas-vs-skimage-vs-opencv-part1/

Smoothing   
- `+1`, additive smoothing, Laplace<sup>라플라스</sup> smoothing

# 활용 예제
## 뉴스 카테고리 분류
Kaggle의 뉴스 데이타 이용, 뉴스 제목을 이용해 카테고리를 분류하는 실험 진행. one-hot vector라 decision tree 룰을 수정하는게 의미 없고, random forest 또한 화이트박스 보단 블랙박스에 더 가깝다. multinomial naive bayes에서 90%가 넘는 가장 높은 정확도 기록[^15] 실행 속도도 NB가 가장 빠르다.

[^15]: https://nbviewer.jupyter.org/github/likejazz/likejazz.github.io/blob/master/public/notebooks/news-classification.ipynb

깃헙 노트북에는 lime 그래프가 보이지 않음. nbviewer를 통하면 잘 보인다[^15]

naive bayes간 비교도 진행해보았는데 마찬가지로 multinomial nb가 가장 빠르며 가장 정확. bernoulli nb가 아주 약간 더 높으나 변별력 없는 수치. gaussian nb의 경우 수식이 복잡해 가장 느림에도 불구하고 가장 낮은 정확도 기록[^16]

[^16]: https://nbviewer.jupyter.org/github/likejazz/likejazz.github.io/blob/master/public/notebooks/news-classification-nb.ipynb